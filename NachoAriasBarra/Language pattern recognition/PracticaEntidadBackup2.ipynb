{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Práctica 1. Diferenciación de entidades.\n",
    "## Autores: \n",
    "     Raúl Sánchez Martín\n",
    "     Ignacio Arias Barra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re, pprint, os, numpy\n",
    "# -*- coding: utf-8 -*-\n",
    "import nltk\n",
    "import string\n",
    "#path_to_append = '/media/nacho/f8371289-0f00-4406-89db-d575f3cdb35e/Master/Trimestre 2/RIM/nltk_data'\n",
    "#nltk.data.path.append(path_to_append)\n",
    "from sklearn.metrics.cluster import *\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from nltk.cluster import GAAClusterer\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from nltk.corpus import stopwords\n",
    "import operator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Código origen\n",
    "Lectura de fichero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    myfile = open(file,\"r\")\n",
    "    data = \"\"\n",
    "    lines = myfile.readlines()\n",
    "    for line in lines:\n",
    "        data = data + line\n",
    "    myfile.close\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cluster_texts(texts, clustersNumber, distance):\n",
    "    # Convierte textos en una coleccion\n",
    "    # Load the list of texts into a TextCollection object.\n",
    "    collection = nltk.TextCollection(texts)    \n",
    "#     print(\"Created a collection of\", len(collection), \"terms.\")\n",
    "    \n",
    "    # Get a list of unique terms\n",
    "    unique_terms = list(set(collection))\n",
    "#     print(\"Unique terms found: \", len(unique_terms))\n",
    "\n",
    "    ### And here we actually call the function and create our array of vectors.\n",
    "    # TF mide la frecuencia en los textos.\n",
    "    # Mira de los terminos unicos, cuantas veces aparece en el documento. No mira cuantas veces aparece en la coleccion\n",
    "    # Hay otras medidas, como TF-IDF que son mas precisas porque tambien miran cuantas veces aparece en la coleccion\n",
    "    vectors = [numpy.array(TF(f,unique_terms, collection)) for f in texts]\n",
    "    # print(\"Vectors created.\")\n",
    "    # print(vectors)\n",
    "    \n",
    "#     for vector in vectors:\n",
    "#         print(\"Vector \", len(vector))\n",
    "\n",
    "#     # initialize the clusterer\n",
    "#     clusterer = GAAClusterer(clustersNumber)\n",
    "#     clusters = clusterer.cluster(vectors, True)\n",
    "    # Estas lineas siguientes comentadas es lo mismo pero con otra libreria, la llamada scikit-learn\n",
    "    clusterer = AgglomerativeClustering(n_clusters=clustersNumber,\n",
    "                                     linkage=\"average\", affinity=distanceFunction)\n",
    "    clusters = clusterer.fit_predict(vectors)\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Frecuencia de términos únicos en un documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Function to create a TF vector for one document. For each of\n",
    "# our unique words, we have a feature which is the tf for that word\n",
    "# in the current document\n",
    "def TF(document, unique_terms, collection):\n",
    "    word_tf = []\n",
    "    for word in unique_terms:\n",
    "        word_tf.append(collection.tf(word, document))\n",
    "    return word_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Nuevo código\n",
    "\n",
    "*FUNCIÓN PARA DETECTAR EL IDIOMA DE UN TEXTO*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_language(possible_lan, text):\n",
    "    # More info in: http://blog.alejandronolla.com/2013/05/15/detecting-text-language-with-python-and-nltk/\n",
    "    languages_score = {}\n",
    "    for language in possible_lan:\n",
    "        stopwords_set = set(stopwords.words(language))\n",
    "        words_set = set(text)\n",
    "        common_elements = words_set.intersection(stopwords_set)\n",
    "        languages_score[possible_lan[language]] = len(common_elements)\n",
    "    return max(languages_score.items(), key=operator.itemgetter(1))[0]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*EVALUACIÓN DIFERENTES TAMAÑOS DE CLUSTER*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def eval_test(top_test, priori_groups, reference):\n",
    "    score = 0\n",
    "    curr_score = -9999999\n",
    "    curr_priori_groups = 1\n",
    "    for n in range(top_test):\n",
    "        test = cluster_texts(texts,priori_groups,distanceFunction)\n",
    "        # print(\"test: \", test)\n",
    "        # Evaluation\n",
    "        score = adjusted_rand_score(reference, test)\n",
    "\n",
    "        print(\"group: \", priori_groups)\n",
    "        print(\"rand_score: \", score)\n",
    "\n",
    "        if score >= curr_score:\n",
    "            curr_score = score\n",
    "            curr_priori_groups = priori_groups\n",
    "\n",
    "        priori_groups+=1\n",
    "\n",
    "    # Gold Standard\n",
    "    # el documento que meto el primero sera igual que los que esten en todos los ceros\n",
    "    print(\"\\nBest agroupation: \", curr_priori_groups)\n",
    "    print(\"Best score: \", curr_score)\n",
    "    print(\"reference: \", reference)\n",
    "    return curr_priori_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*USO DE ENTIDADES I*\n",
    "\n",
    "Se hará uso de las entidades reconocidas utilizando el método *ne_chunk* de la propia librería *nltk*. Simplemente se diferenciará entre nombres, adjetivos, adverbios, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_named_entities_1(initial_document, selected_types):\n",
    "    named_entities = list()\n",
    "    selected_entities = list()\n",
    "    try:\n",
    "        for sentence in initial_document:\n",
    "            tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "            # tagged_sentence = nltk.pos_tag(tokenized_sentence)\n",
    "            tagged_sentence = nltk.pos_tag(tokenized_sentence, tagset= 'universal')\n",
    "            named_ent = nltk.ne_chunk(tagged_sentence, binary=False)\n",
    "            named_entities.append(named_ent)\n",
    "\n",
    "        for element in named_entities:\n",
    "            word = element.pos()[0][0][0]\n",
    "            type_word = element.pos()[0][0][1]\n",
    "            if type_word in selected_types:\n",
    "                selected_entities.append(word)\n",
    "        \n",
    "        return selected_entities\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "        \n",
    "def get_named_ent_txts_1(raw_texts):\n",
    "    named_ent_txts_1 = []\n",
    "    for text in raw_texts:\n",
    "        curr_named_ent = get_named_entities_1(text, types_included_1)\n",
    "        text = nltk.Text(curr_named_ent)\n",
    "        named_ent_txts_1.append(curr_named_ent)\n",
    "    return named_ent_txts_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*ELIMINAR PALABRAS VACÍAS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# # Abrimos el archivo.\n",
    "# with open('./Thomas_Baker/005.txt', 'r', encoding=\"utf8\") as f:\n",
    "# # with open('./Thomas_Baker/036.txt', 'r', encoding=\"utf8\") as f:\n",
    "#     text = f.read()\n",
    "# f.close()\n",
    "\n",
    "\n",
    "def delete_stopwords(text,lan='en'):\n",
    "\n",
    "    if lan == 'en':        \n",
    "        # Obtenemos las sentencias del texto.\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        # Obtenemos las \"stopwords\" del inglés.\n",
    "        stop = set(stopwords.words('english'))\n",
    "\n",
    "    elif lan == 'es':        \n",
    "        # Elegimos el tokenizador para el español de la NLTK.\n",
    "        spanish_tokenizer = nltk.data.load('tokenizers/punkt/spanish.pickle')\n",
    "        # Obtenemos las sentencias del texto.\n",
    "        sentences = spanish_tokenizer.tokenize(text)        \n",
    "\n",
    "        # Obtenemos las \"stopwords\" del español.\n",
    "        stop = set(stopwords.words('spanish'))\n",
    "      \n",
    "    print(\"Frases originales: \")\n",
    "    print(sentences)\n",
    "    \n",
    "    stopw = []\n",
    "    for i in stop:\n",
    "        stopw.append(i)\n",
    "    print(\"Las stop words en lan = \" + lan)\n",
    "    print(stopw)\n",
    "    print()\n",
    "\n",
    "    # Eliminamos las palabras que coinciden con alguna \"stopword\".\n",
    "    print(\"Frases procesadas sin stopwords: \")\n",
    "    for sentence in sentences:\n",
    "        non_stop_sentence = \"\"\n",
    "        for word in sentence.lower().split():\n",
    "            if word not in stop:\n",
    "                non_stop_sentence = non_stop_sentence + word + \" \"\n",
    "        # Imprimimos las sentencias procesadas.\n",
    "        print(non_stop_sentence)\n",
    "    return non_stop_sentence\n",
    "\n",
    "# espanol = delete_stopwords(text,lan='es')\n",
    "# ingles = delete_stopwords(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*ELIMINAR EL NOMBRE DE LA PERSONA EN LOS TEXTOS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*REPRESENTACIÓN DE NGRAMAS A NIVEL DE PALABRAS*\n",
    "\n",
    "(trigramas es poner casa-perro-coche en las ocurrencias por texto),\n",
    " se puede hacer los ngramas con nltk y scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*STEAMMING*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# # Abrimos el archivo.\n",
    "# with open('./Thomas_Baker/005.txt', 'r', encoding=\"utf8\") as f:\n",
    "# with open('./Thomas_Baker/036.txt', 'r', encoding=\"utf8\") as f:\n",
    "#     text = f.read()\n",
    "# f.close()\n",
    "\n",
    "def stemming_original(text, lan='en'):\n",
    "    stemmeds = []\n",
    "    if lan == 'en':\n",
    "        # Steamer ingles\n",
    "        stemmer = PorterStemmer()        \n",
    "    elif lan == 'es':\n",
    "        # Stemer espanol        \n",
    "        stemmer = SnowballStemmer(\"spanish\")\n",
    "    \n",
    "    # Obtenemos los tokens del texto.\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "        \n",
    "    # Para cada token del texto obtenemos su raíz.\n",
    "    for token in tokens:\n",
    "        stemmed = stemmer.stem(token)\n",
    "        stemmeds.append(stemmed)\n",
    "    # Escribimos el resultado para compararlo con las palabras originales.\n",
    "    print(\"Palabras originales: \")\n",
    "    print(tokens)\n",
    "    print(\"Palabras procesadas con Snowball: \")\n",
    "    print(stemmeds)\n",
    "    \n",
    "    \n",
    "def stemming_original(text, lan='en'):\n",
    "    stemmeds = []\n",
    "    if lan == 'en':\n",
    "        # Steamer ingles\n",
    "        stemmer = PorterStemmer()        \n",
    "    elif lan == 'es':\n",
    "        # Stemer espanol        \n",
    "        stemmer = SnowballStemmer(\"spanish\")\n",
    "    \n",
    "    # Obtenemos los tokens del texto.\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "        \n",
    "    # Para cada token del texto obtenemos su raíz.\n",
    "    for token in tokens:\n",
    "        stemmed = stemmer.stem(token)\n",
    "        stemmeds.append(stemmed)\n",
    "    # Escribimos el resultado para compararlo con las palabras originales.\n",
    "    return stemmeds\n",
    "\n",
    "\n",
    "possible_lan = {\"english\":\"en\", \"spanish\":\"es\"}\n",
    "def get_stemmed_txts(raw_texts):\n",
    "    stemmed_txts = []\n",
    "    for text in raw_texts:\n",
    "        language = get_language(possible_lan, text)\n",
    "        stemmed_txt = stemming_original(text, language)\n",
    "        text = nltk.Text(stemmed_txt)\n",
    "        stemmed_txts.append(text)\n",
    "    return stemmed_txts\n",
    "    \n",
    "# stemming(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*EXPLORAR CONTENIDO DE LAS PÁGINAS WEB*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas del propio enunciado de la práctica (muchas incluidas ya antes pero simmplemente para comprobar):\n",
    "\n",
    "* Eliminar palabras vacías\n",
    "* Stemming\n",
    "* Lematización\n",
    "* Reconocer entidades nombradas\n",
    "* n-gramas\n",
    "* entity linking (buscando información de los individus en recursos externos como Wikipedia u otros)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Inicio programa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared  19  documents...\n",
      "They can be accessed using texts[0] - texts[18]\n",
      "steaming text...\n",
      "steaming text...\n",
      "steaming text...\n",
      "steaming text...\n",
      "steaming text...\n",
      "steaming text...\n",
      "steaming text...\n",
      "steaming text...\n",
      "steaming text...\n",
      "steaming text...\n",
      "steaming text...\n",
      "steaming text...\n",
      "steaming text...\n",
      "steaming text...\n",
      "steaming text...\n",
      "steaming text...\n",
      "steaming text...\n",
      "steaming text...\n",
      "steaming text...\n",
      "lenght stemmed txts  19\n",
      "reference:  [0, 1, 2, 0, 0, 0, 3, 0, 0, 0, 2, 0, 3, 3, 0, 1, 2, 0, 1]\n",
      "Model  identity_analysis_1 ; rand_score =  0.14027149321266968\n",
      "Model  primitive ; rand_score =  0.17176317501626542\n",
      "Model  stemmed_txts ; rand_score =  0.17176317501626542\n"
     ]
    }
   ],
   "source": [
    "# Folder with all texts\n",
    "folder = \"Thomas_Baker\"\n",
    "# Empty list to hold text documents.\n",
    "raw_texts = []\n",
    "raw_texts_2 = []\n",
    "named_ent_txts_1 = []\n",
    "types_included_1 = ['NOUN']\n",
    "\n",
    "possible_lan = {\"english\":\"en\", \"spanish\":\"es\"}\n",
    "\n",
    "listing = os.listdir(folder)\n",
    "for file in listing:\n",
    "    if file.endswith(\".txt\"):\n",
    "        url = folder+\"/\"+file\n",
    "        f = open(url,encoding=\"latin-1\");\n",
    "        raw = f.read()\n",
    "        f.close()\n",
    "        raw_texts_2.append(raw)\n",
    "        tokens = nltk.word_tokenize(raw)\n",
    "        text = nltk.Text(tokens)\n",
    "        raw_texts.append(text)\n",
    "\n",
    "print(\"Prepared \", len(raw_texts), \" documents...\")\n",
    "print(\"They can be accessed using texts[0] - texts[\" + str(len(raw_texts)-1) + \"]\")\n",
    "\n",
    "named_ent_txts_1 = get_named_ent_txts_1(raw_texts)\n",
    "stemmed_txts = get_stemmed_txts(raw_texts_2)\n",
    "print(\"lenght stemmed txts \", len(stemmed_txts))\n",
    "\n",
    "\n",
    "# for text in raw_texts:\n",
    "#     print(\"Length \", text[0:10])\n",
    "\n",
    "# Similarity distance\n",
    "distanceFunction =\"cosine\"\n",
    "# distanceFunction = \"euclidean\"\n",
    "\n",
    "\n",
    "# iterations for the evaluation of clusters\n",
    "top_test = 10\n",
    "priori_groups = 1\n",
    "\n",
    "# Final solution for comparing in tests\n",
    "# reference = [0, 1, 2, 0, 0, 0, 3, 0, 0, 0, 2, 0, 3, 3, 0, 1, 2, 0, 1]\n",
    "#groups = eval_test(top_test, priori_groups, reference)\n",
    "\n",
    "\n",
    "tested_models = {}\n",
    "tested_models[\"primitive\"] = cluster_texts(raw_texts,4,distanceFunction)\n",
    "tested_models[\"identity_analysis_1\"] = cluster_texts(named_ent_txts_1,4,distanceFunction)\n",
    "tested_models[\"stemmed_txts\"] = cluster_texts(stemmed_txts,4,distanceFunction)\n",
    "\n",
    "\n",
    "# test = cluster_texts(raw_texts,4,distanceFunction)\n",
    "# print(\"test: \", test)\n",
    "# Gold Standard\n",
    "reference =[0, 1, 2, 0, 0, 0, 3, 0, 0, 0, 2, 0, 3, 3, 0, 1, 2, 0, 1]\n",
    "print(\"reference: \", reference)\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "for model in tested_models:\n",
    "    print(\"Model \", model, \"; rand_score = \", adjusted_rand_score(reference,tested_models[model]))\n",
    "\n",
    "#print(\"rand_score: \", adjusted_rand_score(reference,test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# En la memoria se ha de justificar cada cambio propuesto.\n",
    "\n",
    "# Se ha de comentar los resultados obtenidos.\n",
    "\n",
    "# Siempre buscando una mejora en la clusterización"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:datascience]",
   "language": "python",
   "name": "conda-env-datascience-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
