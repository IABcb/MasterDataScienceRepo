{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Práctica: People Name Disambiguation.\n",
    "## Búsqueda y Recuperación de Información\n",
    "## Autores: \n",
    "     Raúl Sánchez Martín\n",
    "     Ignacio Arias Barra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Índice\n",
    "    ### Importación de librerías\n",
    "    ### Funciones\n",
    "            * Código origen\n",
    "                LECTURA DE FICHERO\n",
    "                CLUSTERING\n",
    "                FRECUENCIA DE TTÉRMINOS ÚNICOS DE UN DOCUMENTO\n",
    "            * Nuevo código\n",
    "                FUNCIÓN PARA DETECTAR EL IDIOMA DE UN TEXTO\n",
    "                USO DE ENTIDADES I\n",
    "                USO DE ENTIDADES II (STANFORD)\n",
    "                ELIMINAR PALABRAS VACÍAS\n",
    "                ELIMINAR EL NOMBRE DE LA PERSONA EN LOS TEXTOS\n",
    "                REPRESENTACIÓN DE NGRAMAS A NIVEL DE PALABRAS\n",
    "                STEAMMING\n",
    "                ELIMINAR PALABRAS REPETIDAS\n",
    "                LEMATIZAR LOS TEXTOS\n",
    "    ### Inicio de programa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re, pprint, os, numpy\n",
    "# -*- coding: utf-8 -*-\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "import goslate\n",
    "from nltk.collocations import *\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "path_to_append = '/media/nacho/f8371289-0f00-4406-89db-d575f3cdb35e/Master/Trimestre 2/RIM/nltk_data'\n",
    "# path_to_append = '/media/raul/Data/nltk_data'\n",
    "# path_to_append = '/home/raul/nltk_data'\n",
    "nltk.data.path.append(path_to_append)\n",
    "from sklearn.metrics.cluster import *\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans, MiniBatchKMeans\n",
    "from nltk.cluster import GAAClusterer\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from nltk.corpus import stopwords\n",
    "import operator\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from nltk.tag import StanfordNERTagger\n",
    "import csv   \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Código origen\n",
    "\n",
    "LECTURA DE FICHERO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    myfile = open(file,\"r\")\n",
    "    data = \"\"\n",
    "    lines = myfile.readlines()\n",
    "    for line in lines:\n",
    "        data = data + line\n",
    "    myfile.close\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cluster_texts(texts, clustersNumber, distanceFunction, clusterMode):\n",
    "    # Convierte textos en una coleccion\n",
    "    # Load the list of texts into a TextCollection object.\n",
    "    collection = nltk.TextCollection(texts)    \n",
    "#     print(\"Created a collection of\", len(collection), \"terms.\")\n",
    "    \n",
    "    # Get a list of unique terms\n",
    "    unique_terms = list(set(collection))\n",
    "#     print(\"Unique terms found: \", len(unique_terms))\n",
    "\n",
    "    ### And here we actually call the function and create our array of vectors.\n",
    "    # TF mide la frecuencia en los textos.\n",
    "    # Mira de los terminos unicos, cuantas veces aparece en el documento. No mira cuantas veces aparece en la coleccion\n",
    "    # Hay otras medidas, como TF-IDF que son mas precisas porque tambien miran cuantas veces aparece en la coleccion\n",
    "    vectors = [numpy.array(TF(f,unique_terms, collection)) for f in texts]\n",
    "    # print(\"Vectors created.\")\n",
    "    # print(vectors)\n",
    "    \n",
    "#     for vector in vectors:\n",
    "#         print(\"Vector \", len(vector))\n",
    "\n",
    "#     # initialize the clusterer\n",
    "#     clusterer = GAAClusterer(clustersNumber)\n",
    "#     clusters = clusterer.cluster(vectors, True)\n",
    "    # Estas lineas siguientes comentadas es lo mismo pero con otra libreria, la llamada scikit-learn\n",
    "    \n",
    "    if clusterMode == \"AgglomerativeClustering\":\n",
    "    \n",
    "        clusterer = AgglomerativeClustering(n_clusters=clustersNumber,\n",
    "                                         linkage=\"average\", affinity=distanceFunction)\n",
    "        clusters = clusterer.fit_predict(vectors)\n",
    "    \n",
    "    elif clusterMode == \"KMeans\":\n",
    "    \n",
    "        clusterer = KMeans(n_clusters=clustersNumber, random_state=0)\n",
    "        clusters = clusterer.fit(vectors).predict(vectors)\n",
    "        \n",
    "    elif clusterMode == \"MiniBatchKMeans\":\n",
    "        \n",
    "        clusterer = MiniBatchKMeans(n_clusters=clustersNumber, random_state=0)\n",
    "        clusters = clusterer.fit(vectors).predict(vectors)\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "FRECUENCIA DE TÉRMINOS ÚNICOS DE UN DOCUMENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Function to create a TF vector for one document. For each of\n",
    "# our unique words, we have a feature which is the tf for that word\n",
    "# in the current document\n",
    "def TF(document, unique_terms, collection):\n",
    "    word_tf = []\n",
    "    for word in unique_terms:\n",
    "        word_tf.append(collection.tf(word, document))\n",
    "    return word_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Nuevo código\n",
    "\n",
    "*FUNCIÓN PARA DETECTAR EL IDIOMA DE UN TEXTO*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_language(possible_lan, text):\n",
    "    # More info in: http://blog.alejandronolla.com/2013/05/15/detecting-text-language-with-python-and-nltk/\n",
    "    languages_score = {}\n",
    "    for language in possible_lan:\n",
    "        stopwords_set = set(stopwords.words(language))\n",
    "        words_set = set(text)\n",
    "        common_elements = words_set.intersection(stopwords_set)\n",
    "        languages_score[possible_lan[language]] = len(common_elements)\n",
    "    return max(languages_score.items(), key=operator.itemgetter(1))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*USO DE ENTIDADES I*\n",
    "\n",
    "Se hará uso de las entidades reconocidas utilizando el método *ne_chunk* de la propia librería *nltk*. Simplemente se diferenciará entre nombres, adjetivos, adverbios, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_named_entities_1(initial_document, selected_types):\n",
    "    named_entities = list()\n",
    "    selected_entities = list()\n",
    "    try:\n",
    "        for sentence in initial_document:\n",
    "            tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "            # tagged_sentence = nltk.pos_tag(tokenized_sentence)\n",
    "            tagged_sentence = nltk.pos_tag(tokenized_sentence, tagset= 'universal')\n",
    "            named_ent = nltk.ne_chunk(tagged_sentence, binary=False)\n",
    "            named_entities.append(named_ent)\n",
    "\n",
    "        for element in named_entities:\n",
    "            word = element.pos()[0][0][0]\n",
    "            type_word = element.pos()[0][0][1]\n",
    "            if type_word in selected_types:\n",
    "                selected_entities.append(word)\n",
    "        \n",
    "        return selected_entities\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "def get_named_entities_2(initial_document):\n",
    "    named_entities = list()\n",
    "    selected_entities = list()\n",
    "    try:\n",
    "        for sentence in initial_document:\n",
    "            tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "            tagged_sentence = nltk.pos_tag(tokenized_sentence)\n",
    "#             tagged_sentence = nltk.pos_tag(tokenized_sentence, tagset= 'universal')\n",
    "            named_ent = nltk.ne_chunk(tagged_sentence, binary=False)\n",
    "            named_entities.append(named_ent)\n",
    "        \n",
    "        for element in named_entities:\n",
    "            if hasattr(element[0], 'label') and element[0].label:\n",
    "                selected_entities.append(element[0].leaves()[0][0])\n",
    "        \n",
    "        return selected_entities \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "        \n",
    "def get_named_ent_txts_1(raw_texts):\n",
    "    named_ent_txts_1 = []\n",
    "    for text in raw_texts:\n",
    "        curr_named_ent = get_named_entities_1(text, types_included_1)\n",
    "        text_to_append = nltk.Text(curr_named_ent)\n",
    "        named_ent_txts_1.append(text_to_append)\n",
    "    return named_ent_txts_1\n",
    "\n",
    "\n",
    "def get_named_ent_txts_2(raw_texts):\n",
    "    named_ent_txts_2 = []\n",
    "    for text in raw_texts:\n",
    "        curr_named_ent = get_named_entities_2(text)\n",
    "        text_to_append = nltk.Text(curr_named_ent)\n",
    "        named_ent_txts_2.append(text_to_append)\n",
    "    return named_ent_txts_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*USO DE ENTIDADES II (STANFORD)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_entities_standorf(sample, types_named_entities):\n",
    "    # Select the first classifier model\n",
    "    stanford_classifier = os.environ.get('STANFORD_MODELS').split(':')[2]\n",
    "\n",
    "    # Get the path for the StandorfNERTagger\n",
    "    stanford_ner_path = os.environ.get('CLASSPATH').split(':')[0]\n",
    "    \n",
    "    st = StanfordNERTagger(stanford_classifier, stanford_ner_path, encoding='utf-8')\n",
    "    result_named_entities = st.tag(sample.split())\n",
    "    filtered_named_entities = []\n",
    "\n",
    "    for item in result_named_entities:\n",
    "        word, entity = item\n",
    "        if entity in types_named_entities:\n",
    "#             filtered_named_entities.append((word, entity))\n",
    "            filtered_named_entities.append(word)\n",
    "        \n",
    "    return filtered_named_entities\n",
    "\n",
    "\n",
    "def get_named_ent_txts_3(raw_texts, types_named_entities):\n",
    "    named_ent_txts_3 = []\n",
    "    for text in raw_texts:\n",
    "        curr_named_ent = get_entities_standorf(text, types_named_entities)\n",
    "        text_to_append = nltk.Text(curr_named_ent)\n",
    "        named_ent_txts_3.append(text_to_append)\n",
    "    return named_ent_txts_3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*ELIMINAR PALABRAS VACÍAS*\n",
    "\n",
    "\n",
    "La siguiente función eliminará las palabras vacías de cada texto. Definimos palabras vacías a palabras propias del léxico que en principio no tienen un significado, como determinantes, artículos..\n",
    "\n",
    "Para ello usamos la librería stopwords del corpus de nltk.\n",
    "\n",
    "Una vez detectado el idioma, le pasamos como parámetros el texto y el idioma del texto para que nos elimine dichas palabras.\n",
    "\n",
    "La función extraerá los tokens (palabras) de cada texto, las comparará con los stopwords de cada idioma y si alguna coincide con una palabra vacía, la eliminará.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def delete_stopwords(text,lan='en'):\n",
    "\n",
    "    if lan == 'en':        \n",
    "        # Obtenemos las sentencias del texto.\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        # Obtenemos las \"stopwords\" del inglés.\n",
    "        stop = set(stopwords.words('english'))\n",
    "\n",
    "    elif lan == 'es':        \n",
    "        # Elegimos el tokenizador para el español de la NLTK.\n",
    "        spanish_tokenizer = nltk.data.load('tokenizers/punkt/spanish.pickle')\n",
    "        # Obtenemos las sentencias del texto.\n",
    "        sentences = spanish_tokenizer.tokenize(text)        \n",
    "\n",
    "        # Obtenemos las \"stopwords\" del español.\n",
    "        stop = set(stopwords.words('spanish'))\n",
    "      \n",
    "    print(\"Frases originales: \")\n",
    "    print(sentences)\n",
    "    \n",
    "    stopw = []\n",
    "    for i in stop:\n",
    "        stopw.append(i)\n",
    "    print(\"Las stop words en lan = \" + lan)\n",
    "    print(stopw)\n",
    "    print()\n",
    "\n",
    "    # Eliminamos las palabras que coinciden con alguna \"stopword\".\n",
    "    print(\"Frases procesadas sin stopwords: \")\n",
    "    for sentence in sentences:\n",
    "        non_stop_sentence = \"\"\n",
    "        for word in sentence.lower().split():\n",
    "            if word not in stop:\n",
    "                non_stop_sentence = non_stop_sentence + word + \" \"\n",
    "        # Imprimimos las sentencias procesadas.\n",
    "        print(non_stop_sentence)\n",
    "    return non_stop_sentence\n",
    "\n",
    "def delete_words_from_text(text, words_to_delete):\n",
    "    \n",
    "    words_to_include = []\n",
    "    words_to_delete = [word.lower() for word in words_to_delete]\n",
    "    \n",
    "    for word in text:\n",
    "        if word.lower() not in words_to_delete:\n",
    "            words_to_include.append(word)\n",
    "            \n",
    "    return words_to_include\n",
    "\n",
    "def get_texts_no_stop_words(raw_texts):\n",
    "    filtered_texts = []\n",
    "    for text in raw_texts:\n",
    "        language = {'en':'english', 'es':'spanish'}[get_language(possible_lan, text)]\n",
    "        words_to_exclude = list(set(stopwords.words(language)))\n",
    "        curr_filtered_text = delete_words_from_text(text, words_to_exclude)\n",
    "        filtered_texts.append(nltk.Text(curr_filtered_text))\n",
    "    return filtered_texts  \n",
    "\n",
    "# espanol = delete_stopwords(text,lan='es')\n",
    "# ingles = delete_stopwords(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*ELIMINAR EL NOMBRE DE LA PERSONA EN LOS TEXTOS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Eliminar las palabras ['Thomas', 'Baker'] de los textos\n",
    "\n",
    "def get_texts_exclude_tomas_baker(raw_texts):\n",
    "    filtered_texts = []\n",
    "    for text in raw_texts:\n",
    "        curr_filtered_text = delete_words_from_text(text, ['Thomas', 'Baker'])\n",
    "        filtered_texts.append(nltk.Text(curr_filtered_text))\n",
    "    return filtered_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*REPRESENTACIÓN DE NGRAMAS A NIVEL DE PALABRAS*\n",
    "\n",
    "La siguiente función extrae ngramas de los textos que le pasemos como parámetro.\n",
    "Podemos establecer el límite del ngrama con la variable *“ngranlimit”*.\n",
    "\n",
    "Definimos ngrama como una tupla de n palabras pertenecientes al texto, escogidas teniendo en cuenta la posición entre ciertas palabras del texto y que puede que representen un concepto único. Puesto que el significado de una palabra viene mejor determinado por las palabras que están a su alrededor, la utilizadión de esta técnica nos va ayudar a detectar textos que puedan explicar conceptos parecidos y por lo tanto a la clusterización de los textos.\n",
    "\n",
    "En concreto, en las pruebas que se realizarán más adelante, se usarán los valores de *“ngranlimit=2”* y *“ngranlimit=3”*, dando como resultado bigramas y trigramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Representación de ngramas\n",
    "\n",
    "def get_ngram(raw_texts, ngramlimit):\n",
    "    ngramlist=[]\n",
    "    for text in raw_texts:\n",
    "        ngram_text = nltk.ngrams(text, ngramlimit)\n",
    "        ngramlist.append(nltk.Text(ngram_text))\n",
    "    return ngramlist\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*STEAMMING*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# # Abrimos el archivo.\n",
    "# with open('./Thomas_Baker/005.txt', 'r', encoding=\"utf8\") as f:\n",
    "# with open('./Thomas_Baker/036.txt', 'r', encoding=\"utf8\") as f:\n",
    "#     text = f.read()\n",
    "# f.close()\n",
    "\n",
    "def stemming_original(text, lan='en'):\n",
    "    stemmeds = []\n",
    "    if lan == 'en':\n",
    "        # Steamer ingles\n",
    "        stemmer = PorterStemmer()        \n",
    "    elif lan == 'es':\n",
    "        # Stemer espanol        \n",
    "        stemmer = SnowballStemmer(\"spanish\")\n",
    "    \n",
    "    # Obtenemos los tokens del texto.\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "        \n",
    "    # Para cada token del texto obtenemos su raíz.\n",
    "    for token in tokens:\n",
    "        stemmed = stemmer.stem(token)\n",
    "        stemmeds.append(stemmed)\n",
    "    # Escribimos el resultado para compararlo con las palabras originales.\n",
    "    print(\"Palabras originales: \")\n",
    "    print(tokens)\n",
    "    print(\"Palabras procesadas con Snowball: \")\n",
    "    print(stemmeds)\n",
    "    \n",
    "    \n",
    "def stemming(text, lan='en'):\n",
    "    stemmeds = []\n",
    "    if lan == 'en':\n",
    "        # Steamer ingles\n",
    "        stemmer = PorterStemmer()        \n",
    "    elif lan == 'es':\n",
    "        # Stemer espanol        \n",
    "        stemmer = SnowballStemmer(\"spanish\")\n",
    "    \n",
    "    # Obtenemos los tokens del texto.\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "        \n",
    "    # Para cada token del texto obtenemos su raíz.\n",
    "    for token in tokens:\n",
    "        stemmed = stemmer.stem(token)\n",
    "        stemmeds.append(stemmed)\n",
    "    # Escribimos el resultado para compararlo con las palabras originales.\n",
    "    return stemmeds\n",
    "\n",
    "\n",
    "possible_lan = {\"english\":\"en\", \"spanish\":\"es\"}\n",
    "def get_stemmed_txts(raw_texts):\n",
    "    stemmed_txts = []\n",
    "    for text in raw_texts:\n",
    "        language = get_language(possible_lan, text)\n",
    "        stemmed_txt = stemming(text, language)\n",
    "        text = nltk.Text(stemmed_txt)\n",
    "        stemmed_txts.append(text)\n",
    "    return stemmed_txts\n",
    "\n",
    "def stemming_2(text, lan='en'):\n",
    "    stemmeds = []\n",
    "    if lan == 'en':\n",
    "        # Steamer ingles\n",
    "        stemmer = PorterStemmer()        \n",
    "    elif lan == 'es':\n",
    "        # Stemer espanol        \n",
    "        stemmer = SnowballStemmer(\"spanish\")\n",
    "        \n",
    "    # Para cada token del texto obtenemos su raíz.\n",
    "    for word in text:\n",
    "        stemmed = stemmer.stem(word)\n",
    "        stemmeds.append(stemmed)\n",
    "    # Escribimos el resultado para compararlo con las palabras originales.\n",
    "    return stemmeds\n",
    "\n",
    "def get_stemmed_txts_2(raw_texts):\n",
    "    stemmed_txts = []\n",
    "    for text in raw_texts:\n",
    "        language = get_language(possible_lan, text)\n",
    "        stemmed_txt = stemming_2(text, language)\n",
    "        text = nltk.Text(stemmed_txt)\n",
    "        stemmed_txts.append(text)\n",
    "    return stemmed_txts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*ELIMINAR PALABRAS REPETIDAS*\n",
    "\n",
    "La idea que se pretende conseguir con esta función, es la de dejar los textos con palabras únicas, de tal forma que no influya cuántas veces se repitan o no un determinado grupo de ellas.\n",
    "\n",
    "Podría decirse que comparamos el *“esqueleto”* o *”esquema”* del texto.\n",
    "\n",
    "Para ello, primero pasamos el texto a minúsculas, para que todas las palabras sean iguales y no nos quedemos con la misma palabra con mayúsculas y minúsculas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Pasamos el texto a minúsculas.\n",
    "# Es necesario para no encontrar palabras repetidas que son la misma.\n",
    "def delete_repWords_stopWords(raw_texts):    \n",
    "    deleted_repeated = []\n",
    "    for text in raw_texts:\n",
    "        delete_repeated_texts = []\n",
    "        text = [w.lower() for w in text]  \n",
    "        unique_terms = list(set(text))\n",
    "#         print(\"Número de palabras del texto: \",str(len(text)))\n",
    "#         print(\"Tamaño del vocabulario filtrado: \", str(len(unique_terms)))\n",
    "        deleted_repeated.append(nltk.Text(unique_terms))    \n",
    "    return deleted_repeated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*LEMATIZAR LOS TEXTOS*\n",
    "\n",
    "La siguiente técnica utilizada, será la lematización de los textos.\n",
    "\n",
    "Lematizar consiste en encontrar el lema de cada palabra, es decir, la unidad mínima de significado de las mismas. Obteniendo estas unidades, se puede realizar una clusterización que agrupe textos que expliquen temas parecidos, obteniendo esa explicación a partir de lemas iguales.\n",
    "\n",
    "Para el texto que encontramos en castellano, tenemos que construirnos un lematizador, llamado SpanisLemmatizer. Este lematizador se encargará de comparar las palabras del texto con las existentes en un diccionario externo. Este diccionario contiene palabras y sus lemas, para que sean devueltos una vez llamados en la función.\n",
    "\n",
    "Este código ha sido obtenido de ejercicios en clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class SpanishLemmatizer():\n",
    "    # Abrimos el fichero donde tenemos la información para cada palabra y lo cargamos en un diccionario.\n",
    "    def __init__(self):\n",
    "        with open('./Lemmatizer/lemmatization-es.txt', 'r', encoding=\"utf8\") as f:\n",
    "            self.lemma_dict = {}\n",
    "            for line in f:\n",
    "                if line.strip(): # Evitamos posibles líneas en blanco.\n",
    "                    value, key = line.split(None, 1) # Nos quedamos con los valores clave y valor.\n",
    "                                                     # None implica espacio en blanco.\n",
    "                    key = key.rstrip() # Limpiamos la línea para evitar los caracteres de salto \\n ó \\r.\n",
    "                    self.lemma_dict[key] = value\n",
    "                    self.lemma_dict[value] = value  # Añadimos por si acaso también el valor como clave.\n",
    "\n",
    "    # Obtenemos el lemma para la palabra solicitada si es que se dispone de él. En caso contrario devuelve la palabra.\n",
    "    # Útil en los casos en los que no se haya aplicado un tratamiento previo del texto (stopwords y puntuación).\n",
    "    def lemmatize(self,word):\n",
    "        try:\n",
    "            lemma = self.lemma_dict[word.lower()]\n",
    "        except KeyError:\n",
    "            lemma = word\n",
    "        return lemma\n",
    "\n",
    "def lemmatize_texts(raw_texts, possible_lan):\n",
    "    nlemmas_texts = []\n",
    "    for text in raw_texts:\n",
    "        language = get_language(possible_lan, text)\n",
    "        if language== 'en':\n",
    "            # Seleccionamos el lematizador.\n",
    "            wordnet_lemmatizer = WordNetLemmatizer()\n",
    "            # Obtenemos los tokens de las sentencias.\n",
    "#             tokens = nltk.word_tokenize(text)\n",
    "            \n",
    "            lemmatizeds = []\n",
    "            nlemmas = []\n",
    "\n",
    "            for token in text:                \n",
    "                lemmatized = wordnet_lemmatizer.lemmatize(token)\n",
    "                lemmatizeds.append(lemmatized)\n",
    "                #print('token ',token)\n",
    "                #print('lema ',lemmatized)\n",
    "                # Obtenemos los lemmas consultando la base de datos de WordNet.\n",
    "                list = wordnet.synsets(token)\n",
    "                # Si encontramos alguna palabra relacionada obtenemos sus lemas y nos quedamos con el primero.\n",
    "                if len(list) >= 1:\n",
    "                    lemma = list[0].lemma_names('eng')\n",
    "                    if len(lemma) > 1:\n",
    "                        nlemmas.append(lemma[0])\n",
    "                    else:\n",
    "                        nlemmas.append((token))\n",
    "                # En caso contrario simplemente introducimos en la solución la palabra actual.\n",
    "            else:\n",
    "                nlemmas.append(token)\n",
    "            nlemmas_texts.append(nltk.Text(nlemmas))\n",
    "        elif language== 'es':\n",
    "            lemmatizer = SpanishLemmatizer()\n",
    "\n",
    "            # Obtenemos los tokens del texto.\n",
    "#             tokens = nltk.word_tokenize(text)            \n",
    "            nlemmas = []\n",
    "            lemmatizeds = []\n",
    "            for token in text:                \n",
    "                # Obtenemos los lemmas consultando el archivo de lemmas.\n",
    "                lemmatized = lemmatizer.lemmatize(token)\n",
    "                #print('token ',token)\n",
    "                #print('lema ',lemmatized)\n",
    "                lemmatizeds.append(lemmatized)\n",
    "                # Obtenemos los lemmas consultando la base de datos de WordNet.\n",
    "                list = wordnet.synsets(token, lang='spa')\n",
    "                # Si encontramos alguna palabra relacionada obtenemos sus lemas y nos quedamos con el primero.\n",
    "                if len(list) >= 1:\n",
    "                    lemma = list[0].lemma_names('spa')\n",
    "                    if len(lemma) >= 1:\n",
    "                        nlemmas.append(lemma[0])\n",
    "                    else:\n",
    "                        nlemmas.append(token)\n",
    "                # En caso contrario simplemente introducimos en la solución la palabra actual.\n",
    "                else:\n",
    "                    nlemmas.append(token)\n",
    "            nlemmas_texts.append(nltk.Text(nlemmas))\n",
    "        else:\n",
    "            print('Lenguaje no reconocido')\n",
    "    return nlemmas_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Ideas del propio enunciado de la práctica (muchas incluidas ya antes pero simmplemente para comprobar):\n",
    "\n",
    "* Diferentes algoritmos de clustering\n",
    "* Eliminar palabras vacías\n",
    "* Stemming\n",
    "* Lematización\n",
    "* Reconocer entidades nombradas\n",
    "* Eliminar palabras repetidas\n",
    "* n-gramas\n",
    "* Probar diferentes números de clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Inicio programa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001.txt\n",
      "002.txt\n",
      "005.txt\n",
      "008.txt\n",
      "009.txt\n",
      "011.txt\n",
      "015.txt\n",
      "017.txt\n",
      "020.txt\n",
      "024.txt\n",
      "027.txt\n",
      "028.txt\n",
      "036.txt\n",
      "041.txt\n",
      "047.txt\n",
      "050.txt\n",
      "056.txt\n",
      "072.txt\n",
      "075.txt\n",
      "Prepared  19  documents...\n",
      "They can be accessed using texts[0] - texts[18]\n"
     ]
    }
   ],
   "source": [
    "# Folder with all texts\n",
    "folder = \"Thomas_Baker\"\n",
    "# gsObj=goslate.Goslate()\n",
    "types_named_entities = [\"LOCATION\", \"PERSON\", \"ORGANIZATION\"]\n",
    "# Empty list to hold text documents.\n",
    "raw_texts = []\n",
    "raw_texts_2 = []\n",
    "raw_texts_en = []\n",
    "named_ent_txts_1 = []\n",
    "types_included_1 = ['NOUN']\n",
    "\n",
    "possible_lan = {\"english\":\"en\", \"spanish\":\"es\"}\n",
    "clustering_modes = [\"AgglomerativeClustering\", \"KMeans\", \"MiniBatchKMeans\"]\n",
    "\n",
    "\n",
    "listing = os.listdir(folder)\n",
    "for file in sorted(listing):\n",
    "    if file.endswith(\".txt\"):\n",
    "        url = folder+\"/\"+file\n",
    "        print(file)\n",
    "        f = open(url,encoding=\"latin-1\");\n",
    "        raw = f.read()\n",
    "        f.close()\n",
    "        f2 = open(url, 'r', encoding=\"utf8\")\n",
    "        raw2 = f2.read()\n",
    "        f2.close()\n",
    "        raw_texts_2.append(raw2)\n",
    "        tokens = nltk.word_tokenize(raw)\n",
    "        text = nltk.Text(tokens)\n",
    "        raw_texts.append(text)\n",
    "        \n",
    "\n",
    "print(\"Prepared \", len(raw_texts), \" documents...\")\n",
    "print(\"They can be accessed using texts[0] - texts[\" + str(len(raw_texts)-1) + \"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A continuación se llevará a cabo el cálculo de las técnicas descritas anteriormente en cada función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Stanford NER...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-e17b5b3b2b29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Using Stanford NER...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstanford_ner_txts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_named_ent_txts_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_texts_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes_named_entities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Using Stanford NER removing Thomas Baker...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstanford_ner_no_thomas_baker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_texts_exclude_tomas_baker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstanford_ner_txts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-82d92bad1268>\u001b[0m in \u001b[0;36mget_named_ent_txts_3\u001b[0;34m(raw_texts, types_named_entities)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mnamed_ent_txts_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_texts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mcurr_named_ent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_entities_standorf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes_named_entities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mtext_to_append\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_named_ent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mnamed_ent_txts_3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_to_append\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-82d92bad1268>\u001b[0m in \u001b[0;36mget_entities_standorf\u001b[0;34m(sample, types_named_entities)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_entities_standorf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes_named_entities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Select the first classifier model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mstanford_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'STANFORD_MODELS'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Get the path for the StandorfNERTagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "print('Using Stanford NER...')\n",
    "stanford_ner_txts = get_named_ent_txts_3(raw_texts_2, types_named_entities)\n",
    "\n",
    "print('Using Stanford NER removing Thomas Baker...')\n",
    "stanford_ner_no_thomas_baker = get_texts_exclude_tomas_baker(stanford_ner_txts)\n",
    "\n",
    "print(\"Removing non-stop words....\")\n",
    "texts_no_stop_words = get_texts_no_stop_words(raw_texts)\n",
    "\n",
    "print(\"Removing Thomas Baker from the texts...\")\n",
    "texts_no_thomas_baker = get_texts_exclude_tomas_baker(raw_texts)\n",
    "\n",
    "print(\"Getting text including only named entities according to criteria 1...\")\n",
    "named_ent_txts_1 = get_named_ent_txts_1(raw_texts)\n",
    "\n",
    "print(\"Getting stemmed texts...\")\n",
    "stemmed_txts = get_stemmed_txts_2(raw_texts)\n",
    "\n",
    "print(\"Getting no_tomas_stemmed...\")\n",
    "no_tomas_stemmed_txts = get_stemmed_txts_2(texts_no_thomas_baker)\n",
    "\n",
    "print(\"Getting no_tomas_stemmed_no_stop...\")\n",
    "no_tomas_stemmed_no_stop_txts = get_texts_no_stop_words(no_tomas_stemmed_txts)\n",
    "\n",
    "print(\"Getting no_tomas_stemmed_ent1...\")\n",
    "no_tomas_stemmed_ent1_txts = get_named_ent_txts_1(no_tomas_stemmed_txts)\n",
    "\n",
    "print(\"Getting text including only named entities according to criteria 2...\")\n",
    "named_ent_txts_2 = get_named_ent_txts_2(raw_texts)\n",
    "\n",
    "print(\"Getting text including only named entities according to criteria 2 and excluding the words 'Tomas' and 'Baker'\")\n",
    "named_ent_2_no_tomas_barker_txts = get_texts_exclude_tomas_baker(named_ent_txts_2)\n",
    "\n",
    "print('Getting bigrams in texts')\n",
    "bigrams_texts = get_ngram(raw_texts, 2)\n",
    "\n",
    "print('Getting trigrams in texts')\n",
    "trigrams_texts = get_ngram(raw_texts, 3)\n",
    "\n",
    "print('Lemmatizing texts')\n",
    "lemmatized_texts = lemmatize_texts(raw_texts, possible_lan)\n",
    "\n",
    "print('Removing repeated words')\n",
    "no_repeatedWords_noStopWords = delete_repWords_stopWords(raw_texts)\n",
    "\n",
    "\n",
    "    \n",
    "# named_ent_txts_2 = get_named_entities_2(raw_texts[0])\n",
    "# for item in named_ent_txts_2:\n",
    "#     if hasattr(item[0], 'label') and item[0].label:\n",
    "#         print(item[0].leaves()[0][0])\n",
    "\n",
    "# Similarity distance\n",
    "distanceFunction =\"cosine\"\n",
    "# distanceFunction = \"euclidean\"\n",
    "\n",
    "reference =[0, 1, 2, 0, 0, 0, 3, 0, 0, 0, 2, 0, 3, 3, 0, 1, 2, 0, 1]\n",
    "print(\"reference: \", reference)\n",
    "\n",
    "#------------------------------\n",
    "\n",
    "# text_1 = \"A football player is Raul Gonzalez\"\n",
    "# text_2 = \"Raul Gonzalez is a football player\"\n",
    "# text_3 = \"Data science is the future\"\n",
    "# text_4 = \"The future is data science\"\n",
    "\n",
    "# text_1 = nltk.Text(nltk.word_tokenize(text_1))\n",
    "# text_2 = nltk.Text(nltk.word_tokenize(text_2))\n",
    "# text_3 = nltk.Text(nltk.word_tokenize(text_3))\n",
    "# text_4 = nltk.Text(nltk.word_tokenize(text_4))\n",
    "\n",
    "\n",
    "\n",
    "# new_texts = [text_1, text_2, text_3, text_4]\n",
    "\n",
    "# collection_new = nltk.TextCollection(new_texts)\n",
    "\n",
    "# print(\"Number of times Raul appears \", collection_new.tf(\"Raul\", text_1))\n",
    "\n",
    "# texts_new = [text_1, text_2, text_3, text_4]\n",
    "# new_reference = [0, 0, 1, 1]\n",
    "# new_test = cluster_texts(texts_new, 2, distanceFunction)\n",
    "# print(\"New test \", new_test)\n",
    "# print(\"New rand score \", adjusted_rand_score(new_reference, new_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*CLUSTER FIJO CON 4 GRUPOS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************\n",
      "Getting results for the clustering mode  AgglomerativeClustering\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.744623655914\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.744623655914\n",
      "Model  trigrams ; rand_score =  0.493168510085\n",
      "Model  no_repeated_words ; rand_score =  0.33371040724\n",
      "Model  bigrams ; rand_score =  0.202339986235\n",
      "Model  identity_analysis_1 ; rand_score =  0.161764705882\n",
      "Model  identity_analysis_2 ; rand_score =  0.0632368703108\n",
      "Model  lemmatized ; rand_score =  0.0632368703108\n",
      "Model  no_tomas_stemmed ; rand_score =  -0.0605700712589\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  -0.0925\n",
      "Model  stemmed_txts ; rand_score =  -0.0925\n",
      "Model  no_stop_words ; rand_score =  -0.10854816825\n",
      "Model  no_tomas_baker ; rand_score =  -0.130111524164\n",
      "Model  primitive ; rand_score =  -0.149642160052\n",
      "########################################\n",
      "########################################\n",
      "**************************************\n",
      "Getting results for the clustering mode  MiniBatchKMeans\n",
      "Model  identity_analysis_2 ; rand_score =  0.283519553073\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.283519553073\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  0.175033921303\n",
      "Model  trigrams ; rand_score =  0.161764705882\n",
      "Model  no_stop_words ; rand_score =  0.149253731343\n",
      "Model  no_tomas_stemmed ; rand_score =  0.123473541384\n",
      "Model  primitive ; rand_score =  0.123473541384\n",
      "Model  no_tomas_baker ; rand_score =  0.123473541384\n",
      "Model  stemmed_txts ; rand_score =  0.123473541384\n",
      "Model  lemmatized ; rand_score =  -0.058949624866\n",
      "Model  identity_analysis_1 ; rand_score =  -0.0605700712589\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  -0.0605700712589\n",
      "Model  bigrams ; rand_score =  -0.0830235439901\n",
      "Model  no_repeated_words ; rand_score =  -0.0961538461538\n",
      "########################################\n",
      "########################################\n",
      "**************************************\n",
      "Getting results for the clustering mode  KMeans\n",
      "Model  identity_analysis_2 ; rand_score =  0.230446927374\n",
      "Model  primitive ; rand_score =  0.175033921303\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  0.175033921303\n",
      "Model  no_stop_words ; rand_score =  0.149253731343\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.147039687703\n",
      "Model  no_tomas_baker ; rand_score =  0.123473541384\n",
      "Model  lemmatized ; rand_score =  0.104949314252\n",
      "Model  no_repeated_words ; rand_score =  0.0836012861736\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.0725593667546\n",
      "Model  no_tomas_stemmed ; rand_score =  0.0203527815468\n",
      "Model  stemmed_txts ; rand_score =  0.0203527815468\n",
      "Model  identity_analysis_1 ; rand_score =  -0.0569877883311\n",
      "Model  bigrams ; rand_score =  -0.058949624866\n",
      "Model  trigrams ; rand_score =  -0.058949624866\n",
      "########################################\n",
      "########################################\n"
     ]
    }
   ],
   "source": [
    "tested_models = {}\n",
    "fix_grouping = 4\n",
    "header_fields=['clustering_mode','model','rand_score','cluster_split']\n",
    "\n",
    "csv_file = 'CSV_output/4clusters.csv'\n",
    "with open(csv_file, 'a') as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerow(header_fields)\n",
    "    new_row = []\n",
    "\n",
    "\n",
    "\n",
    "    for cluster_mode in clustering_modes:\n",
    "        tested_models[cluster_mode] = {}\n",
    "        tested_models[cluster_mode][\"primitive\"] = cluster_texts(raw_texts,fix_grouping,distanceFunction, cluster_mode)\n",
    "        tested_models[cluster_mode][\"identity_analysis_1\"] = cluster_texts(named_ent_txts_1,fix_grouping,distanceFunction, cluster_mode)\n",
    "        tested_models[cluster_mode][\"stemmed_txts\"] = cluster_texts(stemmed_txts,fix_grouping,distanceFunction, cluster_mode)\n",
    "        tested_models[cluster_mode][\"no_tomas_baker\"] = cluster_texts(texts_no_thomas_baker,fix_grouping,distanceFunction, cluster_mode)\n",
    "        tested_models[cluster_mode][\"no_stop_words\"] = cluster_texts(texts_no_stop_words,fix_grouping,distanceFunction, cluster_mode)\n",
    "        tested_models[cluster_mode][\"no_tomas_stemmed\"] = cluster_texts(no_tomas_stemmed_txts,fix_grouping,distanceFunction, cluster_mode)\n",
    "        tested_models[cluster_mode][\"no_tomas_stemmed_no_stop\"] = cluster_texts(no_tomas_stemmed_no_stop_txts,fix_grouping,\n",
    "                                                                                distanceFunction, cluster_mode)\n",
    "        tested_models[cluster_mode][\"no_tomas_stemmed_ent1\"] = cluster_texts(no_tomas_stemmed_ent1_txts,fix_grouping,distanceFunction, \n",
    "                                                                             cluster_mode)\n",
    "        tested_models[cluster_mode][\"identity_analysis_2\"] = cluster_texts(named_ent_txts_2,fix_grouping,distanceFunction, cluster_mode)\n",
    "        tested_models[cluster_mode][\"named_ent_2_no_tomas_barker\"] = cluster_texts(named_ent_2_no_tomas_barker_txts,\n",
    "                                                                 fix_grouping,distanceFunction, cluster_mode)\n",
    "        tested_models[cluster_mode]['bigrams'] = cluster_texts(bigrams_texts,fix_grouping,distanceFunction, cluster_mode)\n",
    "        tested_models[cluster_mode]['trigrams'] = cluster_texts(trigrams_texts, fix_grouping,distanceFunction, cluster_mode)\n",
    "        tested_models[cluster_mode]['lemmatized'] = cluster_texts(lemmatized_texts, fix_grouping,distanceFunction, cluster_mode)\n",
    "        tested_models[cluster_mode]['no_repeated_words'] = cluster_texts(no_repeatedWords_noStopWords,fix_grouping, distanceFunction, cluster_mode)\n",
    "        tested_models[cluster_mode]['stanford_ner_txts'] = cluster_texts(stanford_ner_txts,fix_grouping, \n",
    "                                                                         distanceFunction, cluster_mode)\n",
    "        tested_models[cluster_mode]['stanford_ner_no_thomas_baker'] = cluster_texts(stanford_ner_no_thomas_baker,fix_grouping, \n",
    "                                                                         distanceFunction, cluster_mode)\n",
    "\n",
    "    # Evaluation\n",
    "    tested_models_scores = {}\n",
    "\n",
    "    for cluster_mode in tested_models:\n",
    "        tested_models_scores[cluster_mode] = {}\n",
    "        for model in tested_models[cluster_mode]:\n",
    "            tested_models_scores[cluster_mode][model] = adjusted_rand_score(reference,tested_models[cluster_mode][model])\n",
    "    #     print(\"Model \", model, \"; rand_score = \", adjusted_rand_score(reference,tested_models[model]))\n",
    "\n",
    "    for cluster_mode in tested_models_scores:\n",
    "        print(\"**************************************\")\n",
    "        print(\"Getting results for the clustering mode \", cluster_mode)\n",
    "        for model in sorted(tested_models_scores[cluster_mode].items(), key=operator.itemgetter(1), reverse=True):\n",
    "            print(\"Model \", model[0], \"; rand_score = \", model[1])\n",
    "            new_row = [cluster_mode, model[0], model[1], fix_grouping]\n",
    "            writer.writerow(new_row)\n",
    "        print(\"########################################\")\n",
    "        print(\"########################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*DIFERENTE NÚMERO DE CLUSTERS*\n",
    "\n",
    "Como se ha comentado, el número perfecto de clusterización se sabe de antemano, siendo éste de 4. Sin embargo, un bueno algoritmo de clusterización automática debería poder sacar el número óptimo sin tener que indicárselo previamente. Por ello, utilizamos la última ténica. \n",
    "\n",
    "En esta técnica, volveremos a ejecutar los 3 algoritmos de clusterización y todas las técnicas previamente utililzadas pero en este caso, variando el número de clusters a priori desde 0 hasta 10. Así compararemos si las técnicas utilizadas dan buenos resultados en la clusterización de 4 frente al resto de números. Esto indicará que esa técnica o técnicas son las que dan mejores resultados, ya que el número óptimo de grupos lo conocemos y es de 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASIFICATION WITH 1 CLUSTERS\n",
      "**************************************\n",
      "Getting results for the clustering mode  AgglomerativeClustering\n",
      "Model  bigrams ; rand_score =  0.0\n",
      "Model  no_tomas_stemmed ; rand_score =  0.0\n",
      "Model  primitive ; rand_score =  0.0\n",
      "Model  no_repeated_words ; rand_score =  0.0\n",
      "Model  identity_analysis_2 ; rand_score =  0.0\n",
      "Model  no_tomas_baker ; rand_score =  0.0\n",
      "Model  trigrams ; rand_score =  0.0\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.0\n",
      "Model  no_stop_words ; rand_score =  0.0\n",
      "Model  lemmatized ; rand_score =  0.0\n",
      "Model  identity_analysis_1 ; rand_score =  0.0\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.0\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  0.0\n",
      "Model  stemmed_txts ; rand_score =  0.0\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "**************************************\n",
      "Getting results for the clustering mode  MiniBatchKMeans\n",
      "Model  bigrams ; rand_score =  0.0\n",
      "Model  no_tomas_stemmed ; rand_score =  0.0\n",
      "Model  primitive ; rand_score =  0.0\n",
      "Model  no_repeated_words ; rand_score =  0.0\n",
      "Model  identity_analysis_2 ; rand_score =  0.0\n",
      "Model  no_tomas_baker ; rand_score =  0.0\n",
      "Model  trigrams ; rand_score =  0.0\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.0\n",
      "Model  no_stop_words ; rand_score =  0.0\n",
      "Model  lemmatized ; rand_score =  0.0\n",
      "Model  identity_analysis_1 ; rand_score =  0.0\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.0\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  0.0\n",
      "Model  stemmed_txts ; rand_score =  0.0\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "**************************************\n",
      "Getting results for the clustering mode  KMeans\n",
      "Model  bigrams ; rand_score =  0.0\n",
      "Model  no_tomas_stemmed ; rand_score =  0.0\n",
      "Model  primitive ; rand_score =  0.0\n",
      "Model  no_repeated_words ; rand_score =  0.0\n",
      "Model  identity_analysis_2 ; rand_score =  0.0\n",
      "Model  no_tomas_baker ; rand_score =  0.0\n",
      "Model  trigrams ; rand_score =  0.0\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.0\n",
      "Model  no_stop_words ; rand_score =  0.0\n",
      "Model  lemmatized ; rand_score =  0.0\n",
      "Model  identity_analysis_1 ; rand_score =  0.0\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.0\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  0.0\n",
      "Model  stemmed_txts ; rand_score =  0.0\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "CLASIFICATION WITH 2 CLUSTERS\n",
      "**************************************\n",
      "Getting results for the clustering mode  AgglomerativeClustering\n",
      "Model  no_tomas_stemmed ; rand_score =  0.066762041011\n",
      "Model  no_repeated_words ; rand_score =  0.066762041011\n",
      "Model  identity_analysis_2 ; rand_score =  0.066762041011\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.066762041011\n",
      "Model  lemmatized ; rand_score =  0.066762041011\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.066762041011\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  0.066762041011\n",
      "Model  stemmed_txts ; rand_score =  0.066762041011\n",
      "Model  identity_analysis_1 ; rand_score =  -0.00503778337531\n",
      "Model  primitive ; rand_score =  -0.0221852468109\n",
      "Model  no_tomas_baker ; rand_score =  -0.0221852468109\n",
      "Model  no_stop_words ; rand_score =  -0.0221852468109\n",
      "Model  bigrams ; rand_score =  -0.0600858369099\n",
      "Model  trigrams ; rand_score =  -0.0600858369099\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "**************************************\n",
      "Getting results for the clustering mode  MiniBatchKMeans\n",
      "Model  bigrams ; rand_score =  0.066762041011\n",
      "Model  no_tomas_stemmed ; rand_score =  0.066762041011\n",
      "Model  primitive ; rand_score =  0.066762041011\n",
      "Model  no_repeated_words ; rand_score =  0.066762041011\n",
      "Model  no_tomas_baker ; rand_score =  0.066762041011\n",
      "Model  trigrams ; rand_score =  0.066762041011\n",
      "Model  no_stop_words ; rand_score =  0.066762041011\n",
      "Model  lemmatized ; rand_score =  0.066762041011\n",
      "Model  identity_analysis_1 ; rand_score =  0.066762041011\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.066762041011\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  0.066762041011\n",
      "Model  stemmed_txts ; rand_score =  0.066762041011\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.0143112701252\n",
      "Model  identity_analysis_2 ; rand_score =  -0.0572337042925\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "**************************************\n",
      "Getting results for the clustering mode  KMeans\n",
      "Model  bigrams ; rand_score =  0.066762041011\n",
      "Model  no_tomas_stemmed ; rand_score =  0.066762041011\n",
      "Model  primitive ; rand_score =  0.066762041011\n",
      "Model  no_repeated_words ; rand_score =  0.066762041011\n",
      "Model  identity_analysis_2 ; rand_score =  0.066762041011\n",
      "Model  no_tomas_baker ; rand_score =  0.066762041011\n",
      "Model  trigrams ; rand_score =  0.066762041011\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.066762041011\n",
      "Model  no_stop_words ; rand_score =  0.066762041011\n",
      "Model  lemmatized ; rand_score =  0.066762041011\n",
      "Model  identity_analysis_1 ; rand_score =  0.066762041011\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.066762041011\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  0.066762041011\n",
      "Model  stemmed_txts ; rand_score =  0.066762041011\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "CLASIFICATION WITH 3 CLUSTERS\n",
      "**************************************\n",
      "Getting results for the clustering mode  AgglomerativeClustering\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.634146341463\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.514007308161\n",
      "Model  trigrams ; rand_score =  0.275659824047\n",
      "Model  no_repeated_words ; rand_score =  0.239188467699\n",
      "Model  bigrams ; rand_score =  0.0975927130774\n",
      "Model  identity_analysis_2 ; rand_score =  0.0565936999466\n",
      "Model  lemmatized ; rand_score =  0.0010111223458\n",
      "Model  identity_analysis_1 ; rand_score =  0.0010111223458\n",
      "Model  no_tomas_stemmed ; rand_score =  -0.00224466891134\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  -0.00224466891134\n",
      "Model  stemmed_txts ; rand_score =  -0.00224466891134\n",
      "Model  no_stop_words ; rand_score =  -0.0657640232108\n",
      "Model  no_tomas_baker ; rand_score =  -0.0809384164223\n",
      "Model  primitive ; rand_score =  -0.113510178902\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "**************************************\n",
      "Getting results for the clustering mode  MiniBatchKMeans\n",
      "Model  identity_analysis_2 ; rand_score =  0.125701459035\n",
      "Model  bigrams ; rand_score =  0.0010111223458\n",
      "Model  primitive ; rand_score =  0.0010111223458\n",
      "Model  no_repeated_words ; rand_score =  0.0010111223458\n",
      "Model  no_tomas_baker ; rand_score =  0.0010111223458\n",
      "Model  trigrams ; rand_score =  0.0010111223458\n",
      "Model  no_stop_words ; rand_score =  0.0010111223458\n",
      "Model  lemmatized ; rand_score =  0.0010111223458\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  -0.00224466891134\n",
      "Model  identity_analysis_1 ; rand_score =  -0.0363636363636\n",
      "Model  no_tomas_stemmed ; rand_score =  -0.0448478376935\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  -0.0448478376935\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  -0.0448478376935\n",
      "Model  stemmed_txts ; rand_score =  -0.0448478376935\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "**************************************\n",
      "Getting results for the clustering mode  KMeans\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.503267973856\n",
      "Model  lemmatized ; rand_score =  0.239188467699\n",
      "Model  stemmed_txts ; rand_score =  0.147039687703\n",
      "Model  bigrams ; rand_score =  0.0010111223458\n",
      "Model  no_tomas_stemmed ; rand_score =  0.0010111223458\n",
      "Model  primitive ; rand_score =  0.0010111223458\n",
      "Model  no_repeated_words ; rand_score =  0.0010111223458\n",
      "Model  no_tomas_baker ; rand_score =  0.0010111223458\n",
      "Model  trigrams ; rand_score =  0.0010111223458\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  0.0010111223458\n",
      "Model  identity_analysis_1 ; rand_score =  -0.0363636363636\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  -0.0363636363636\n",
      "Model  identity_analysis_2 ; rand_score =  -0.0448478376935\n",
      "Model  no_stop_words ; rand_score =  -0.0448478376935\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "CLASIFICATION WITH 4 CLUSTERS\n",
      "**************************************\n",
      "Getting results for the clustering mode  AgglomerativeClustering\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.744623655914\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.744623655914\n",
      "Model  trigrams ; rand_score =  0.493168510085\n",
      "Model  no_repeated_words ; rand_score =  0.33371040724\n",
      "Model  bigrams ; rand_score =  0.202339986235\n",
      "Model  identity_analysis_1 ; rand_score =  0.161764705882\n",
      "Model  identity_analysis_2 ; rand_score =  0.0632368703108\n",
      "Model  lemmatized ; rand_score =  0.0632368703108\n",
      "Model  no_tomas_stemmed ; rand_score =  -0.0605700712589\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  -0.0925\n",
      "Model  stemmed_txts ; rand_score =  -0.0925\n",
      "Model  no_stop_words ; rand_score =  -0.10854816825\n",
      "Model  no_tomas_baker ; rand_score =  -0.130111524164\n",
      "Model  primitive ; rand_score =  -0.149642160052\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "**************************************\n",
      "Getting results for the clustering mode  MiniBatchKMeans\n",
      "Model  identity_analysis_2 ; rand_score =  0.283519553073\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.283519553073\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  0.175033921303\n",
      "Model  trigrams ; rand_score =  0.161764705882\n",
      "Model  no_stop_words ; rand_score =  0.149253731343\n",
      "Model  no_tomas_stemmed ; rand_score =  0.123473541384\n",
      "Model  primitive ; rand_score =  0.123473541384\n",
      "Model  no_tomas_baker ; rand_score =  0.123473541384\n",
      "Model  stemmed_txts ; rand_score =  0.123473541384\n",
      "Model  lemmatized ; rand_score =  -0.058949624866\n",
      "Model  identity_analysis_1 ; rand_score =  -0.0605700712589\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  -0.0605700712589\n",
      "Model  bigrams ; rand_score =  -0.0830235439901\n",
      "Model  no_repeated_words ; rand_score =  -0.0961538461538\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "**************************************\n",
      "Getting results for the clustering mode  KMeans\n",
      "Model  identity_analysis_2 ; rand_score =  0.230446927374\n",
      "Model  primitive ; rand_score =  0.175033921303\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  0.175033921303\n",
      "Model  no_stop_words ; rand_score =  0.149253731343\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.147039687703\n",
      "Model  no_tomas_baker ; rand_score =  0.123473541384\n",
      "Model  lemmatized ; rand_score =  0.104949314252\n",
      "Model  no_repeated_words ; rand_score =  0.0836012861736\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.0725593667546\n",
      "Model  no_tomas_stemmed ; rand_score =  0.0203527815468\n",
      "Model  stemmed_txts ; rand_score =  0.0203527815468\n",
      "Model  identity_analysis_1 ; rand_score =  -0.0569877883311\n",
      "Model  bigrams ; rand_score =  -0.058949624866\n",
      "Model  trigrams ; rand_score =  -0.058949624866\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "CLASIFICATION WITH 5 CLUSTERS\n",
      "**************************************\n",
      "Getting results for the clustering mode  AgglomerativeClustering\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.816044260028\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.720998531571\n",
      "Model  trigrams ; rand_score =  0.473614775726\n",
      "Model  no_repeated_words ; rand_score =  0.453892215569\n",
      "Model  bigrams ; rand_score =  0.249647390691\n",
      "Model  identity_analysis_2 ; rand_score =  0.226347305389\n",
      "Model  lemmatized ; rand_score =  0.226347305389\n",
      "Model  identity_analysis_1 ; rand_score =  0.0416141235813\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  -0.0277044854881\n",
      "Model  no_tomas_stemmed ; rand_score =  -0.10922787194\n",
      "Model  no_stop_words ; rand_score =  -0.119178082192\n",
      "Model  stemmed_txts ; rand_score =  -0.127968337731\n",
      "Model  primitive ; rand_score =  -0.167320261438\n",
      "Model  no_tomas_baker ; rand_score =  -0.167320261438\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "**************************************\n",
      "Getting results for the clustering mode  MiniBatchKMeans\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.473614775726\n",
      "Model  identity_analysis_2 ; rand_score =  0.309593023256\n",
      "Model  no_repeated_words ; rand_score =  0.249101796407\n",
      "Model  lemmatized ; rand_score =  0.224733207784\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.122691292876\n",
      "Model  identity_analysis_1 ; rand_score =  0.0415430267062\n",
      "Model  bigrams ; rand_score =  0.0224274406332\n",
      "Model  no_stop_words ; rand_score =  -0.0826210826211\n",
      "Model  primitive ; rand_score =  -0.111493461803\n",
      "Model  no_tomas_baker ; rand_score =  -0.111493461803\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  -0.111493461803\n",
      "Model  no_tomas_stemmed ; rand_score =  -0.117647058824\n",
      "Model  stemmed_txts ; rand_score =  -0.117647058824\n",
      "Model  trigrams ; rand_score =  -0.137724550898\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "**************************************\n",
      "Getting results for the clustering mode  KMeans\n",
      "Model  lemmatized ; rand_score =  0.395573997234\n",
      "Model  no_repeated_words ; rand_score =  0.249101796407\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  0.222849083216\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.211618257261\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.122691292876\n",
      "Model  identity_analysis_1 ; rand_score =  0.0979228486647\n",
      "Model  trigrams ; rand_score =  0.0898203592814\n",
      "Model  identity_analysis_2 ; rand_score =  0.0513950073421\n",
      "Model  no_tomas_stemmed ; rand_score =  0.013353115727\n",
      "Model  primitive ; rand_score =  0.013353115727\n",
      "Model  no_tomas_baker ; rand_score =  0.013353115727\n",
      "Model  stemmed_txts ; rand_score =  0.013353115727\n",
      "Model  no_stop_words ; rand_score =  -0.0826210826211\n",
      "Model  bigrams ; rand_score =  -0.111300397501\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "CLASIFICATION WITH 6 CLUSTERS\n",
      "**************************************\n",
      "Getting results for the clustering mode  AgglomerativeClustering\n",
      "Model  no_repeated_words ; rand_score =  0.801945795691\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.798484848485\n",
      "Model  trigrams ; rand_score =  0.785229841748\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.696220930233\n",
      "Model  identity_analysis_2 ; rand_score =  0.314122862571\n",
      "Model  bigrams ; rand_score =  0.1792\n",
      "Model  lemmatized ; rand_score =  0.145427286357\n",
      "Model  identity_analysis_1 ; rand_score =  0.0294117647059\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  -0.0694927032662\n",
      "Model  no_stop_words ; rand_score =  -0.111493461803\n",
      "Model  primitive ; rand_score =  -0.134762633997\n",
      "Model  no_tomas_baker ; rand_score =  -0.134762633997\n",
      "Model  no_tomas_stemmed ; rand_score =  -0.145791915176\n",
      "Model  stemmed_txts ; rand_score =  -0.145791915176\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "**************************************\n",
      "Getting results for the clustering mode  MiniBatchKMeans\n",
      "Model  no_repeated_words ; rand_score =  0.564280750521\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.379430159833\n",
      "Model  no_stop_words ; rand_score =  0.344827586207\n",
      "Model  no_tomas_stemmed ; rand_score =  0.236669101534\n",
      "Model  no_tomas_baker ; rand_score =  0.236669101534\n",
      "Model  stemmed_txts ; rand_score =  0.236669101534\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.194579569145\n",
      "Model  identity_analysis_2 ; rand_score =  0.147058823529\n",
      "Model  primitive ; rand_score =  0.125639152666\n",
      "Model  identity_analysis_1 ; rand_score =  0.116941529235\n",
      "Model  trigrams ; rand_score =  0.0439404677534\n",
      "Model  bigrams ; rand_score =  0.025332488917\n",
      "Model  lemmatized ; rand_score =  -0.057845263919\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  -0.0588235294118\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "**************************************\n",
      "Getting results for the clustering mode  KMeans\n",
      "Model  lemmatized ; rand_score =  0.564280750521\n",
      "Model  identity_analysis_2 ; rand_score =  0.336592178771\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.241011984021\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  0.236669101534\n",
      "Model  identity_analysis_1 ; rand_score =  0.194579569145\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.194579569145\n",
      "Model  no_repeated_words ; rand_score =  0.169727675744\n",
      "Model  no_stop_words ; rand_score =  0.0314842578711\n",
      "Model  bigrams ; rand_score =  0.025332488917\n",
      "Model  trigrams ; rand_score =  0.025332488917\n",
      "Model  no_tomas_stemmed ; rand_score =  0.00299850074963\n",
      "Model  primitive ; rand_score =  0.00299850074963\n",
      "Model  no_tomas_baker ; rand_score =  0.00299850074963\n",
      "Model  stemmed_txts ; rand_score =  0.00299850074963\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "CLASIFICATION WITH 7 CLUSTERS\n",
      "**************************************\n",
      "Getting results for the clustering mode  AgglomerativeClustering\n",
      "Model  no_repeated_words ; rand_score =  0.930606281958\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.782939832445\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.680788897005\n",
      "Model  trigrams ; rand_score =  0.551668022783\n",
      "Model  bigrams ; rand_score =  0.245033112583\n",
      "Model  identity_analysis_2 ; rand_score =  0.16\n",
      "Model  lemmatized ; rand_score =  0.1488\n",
      "Model  identity_analysis_1 ; rand_score =  0.0576\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  -0.0879888268156\n",
      "Model  primitive ; rand_score =  -0.103420843278\n",
      "Model  no_stop_words ; rand_score =  -0.103420843278\n",
      "Model  no_tomas_stemmed ; rand_score =  -0.108949416342\n",
      "Model  no_tomas_baker ; rand_score =  -0.108949416342\n",
      "Model  stemmed_txts ; rand_score =  -0.108949416342\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "**************************************\n",
      "Getting results for the clustering mode  MiniBatchKMeans\n",
      "Model  no_repeated_words ; rand_score =  0.45872899927\n",
      "Model  no_stop_words ; rand_score =  0.245914396887\n",
      "Model  lemmatized ; rand_score =  0.22474916388\n",
      "Model  identity_analysis_1 ; rand_score =  0.128720836685\n",
      "Model  identity_analysis_2 ; rand_score =  0.107981220657\n",
      "Model  bigrams ; rand_score =  0.0976588628763\n",
      "Model  trigrams ; rand_score =  0.0976588628763\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.0883472962681\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.0406932931424\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  0.0406932931424\n",
      "Model  no_tomas_stemmed ; rand_score =  -0.00757575757576\n",
      "Model  primitive ; rand_score =  -0.00757575757576\n",
      "Model  no_tomas_baker ; rand_score =  -0.00757575757576\n",
      "Model  stemmed_txts ; rand_score =  -0.00757575757576\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "**************************************\n",
      "Getting results for the clustering mode  KMeans\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.393774319066\n",
      "Model  bigrams ; rand_score =  0.245914396887\n",
      "Model  no_repeated_words ; rand_score =  0.197183098592\n",
      "Model  lemmatized ; rand_score =  0.188291139241\n",
      "Model  primitive ; rand_score =  0.136363636364\n",
      "Model  no_tomas_stemmed ; rand_score =  0.1184\n",
      "Model  no_tomas_baker ; rand_score =  0.1184\n",
      "Model  stemmed_txts ; rand_score =  0.1184\n",
      "Model  trigrams ; rand_score =  0.0976588628763\n",
      "Model  identity_analysis_1 ; rand_score =  0.0881195908733\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.0881195908733\n",
      "Model  identity_analysis_2 ; rand_score =  0.0477326968974\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  0.0283241542093\n",
      "Model  no_stop_words ; rand_score =  -0.00488201790073\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "CLASIFICATION WITH 8 CLUSTERS\n",
      "**************************************\n",
      "Getting results for the clustering mode  AgglomerativeClustering\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.796324655436\n",
      "Model  no_repeated_words ; rand_score =  0.682577565632\n",
      "Model  trigrams ; rand_score =  0.546502057613\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.395309882747\n",
      "Model  identity_analysis_2 ; rand_score =  0.341801385681\n",
      "Model  bigrams ; rand_score =  0.303495311168\n",
      "Model  identity_analysis_1 ; rand_score =  0.140703517588\n",
      "Model  lemmatized ; rand_score =  0.077366255144\n",
      "Model  no_tomas_stemmed ; rand_score =  -0.0760517799353\n",
      "Model  primitive ; rand_score =  -0.0760517799353\n",
      "Model  no_tomas_baker ; rand_score =  -0.0760517799353\n",
      "Model  no_stop_words ; rand_score =  -0.0760517799353\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  -0.0760517799353\n",
      "Model  stemmed_txts ; rand_score =  -0.0760517799353\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "**************************************\n",
      "Getting results for the clustering mode  MiniBatchKMeans\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.346972176759\n",
      "Model  lemmatized ; rand_score =  0.256826568266\n",
      "Model  no_repeated_words ; rand_score =  0.228782287823\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.1488\n",
      "Model  identity_analysis_2 ; rand_score =  0.0582218725413\n",
      "Model  primitive ; rand_score =  0.0485133020344\n",
      "Model  no_tomas_baker ; rand_score =  0.0485133020344\n",
      "Model  identity_analysis_1 ; rand_score =  0.0477326968974\n",
      "Model  bigrams ; rand_score =  0.0352609308886\n",
      "Model  no_stop_words ; rand_score =  -0.0109546165884\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  -0.0294117647059\n",
      "Model  no_tomas_stemmed ; rand_score =  -0.0336\n",
      "Model  stemmed_txts ; rand_score =  -0.0336\n",
      "Model  trigrams ; rand_score =  -0.0719322990127\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "**************************************\n",
      "Getting results for the clustering mode  KMeans\n",
      "Model  lemmatized ; rand_score =  0.359877488515\n",
      "Model  identity_analysis_2 ; rand_score =  0.228782287823\n",
      "Model  trigrams ; rand_score =  0.228782287823\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.149715215622\n",
      "Model  no_repeated_words ; rand_score =  0.138424821002\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.0689127105666\n",
      "Model  identity_analysis_1 ; rand_score =  0.0671031096563\n",
      "Model  no_stop_words ; rand_score =  0.0655737704918\n",
      "Model  bigrams ; rand_score =  0.0582218725413\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  0.0569568755085\n",
      "Model  no_tomas_stemmed ; rand_score =  0.0460905349794\n",
      "Model  stemmed_txts ; rand_score =  0.0452261306533\n",
      "Model  primitive ; rand_score =  -0.0822784810127\n",
      "Model  no_tomas_baker ; rand_score =  -0.0822784810127\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "CLASIFICATION WITH 9 CLUSTERS\n",
      "**************************************\n",
      "Getting results for the clustering mode  AgglomerativeClustering\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.764705882353\n",
      "Model  no_repeated_words ; rand_score =  0.46547314578\n",
      "Model  identity_analysis_2 ; rand_score =  0.434432823813\n",
      "Model  trigrams ; rand_score =  0.433077578858\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.274305555556\n",
      "Model  bigrams ; rand_score =  0.262295081967\n",
      "Model  identity_analysis_1 ; rand_score =  0.0983606557377\n",
      "Model  lemmatized ; rand_score =  0.0443307757886\n",
      "Model  no_stop_words ; rand_score =  0.0222984562607\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  0.0222984562607\n",
      "Model  no_tomas_stemmed ; rand_score =  -0.0572831423895\n",
      "Model  stemmed_txts ; rand_score =  -0.0572831423895\n",
      "Model  primitive ; rand_score =  -0.110288065844\n",
      "Model  no_tomas_baker ; rand_score =  -0.110288065844\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "**************************************\n",
      "Getting results for the clustering mode  MiniBatchKMeans\n",
      "Model  lemmatized ; rand_score =  0.323529411765\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.259322033898\n",
      "Model  stemmed_txts ; rand_score =  0.247683235046\n",
      "Model  identity_analysis_1 ; rand_score =  0.238704177323\n",
      "Model  no_repeated_words ; rand_score =  0.0981496379726\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.0981496379726\n",
      "Model  identity_analysis_2 ; rand_score =  0.0877483443709\n",
      "Model  no_stop_words ; rand_score =  0.0877483443709\n",
      "Model  no_tomas_stemmed ; rand_score =  0.055602358888\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  0.00169491525424\n",
      "Model  trigrams ; rand_score =  -0.0148367952522\n",
      "Model  primitive ; rand_score =  -0.0241351568785\n",
      "Model  no_tomas_baker ; rand_score =  -0.0241351568785\n",
      "Model  bigrams ; rand_score =  -0.0294117647059\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "**************************************\n",
      "Getting results for the clustering mode  KMeans\n",
      "Model  bigrams ; rand_score =  0.2704\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.259322033898\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.150662251656\n",
      "Model  stemmed_txts ; rand_score =  0.150662251656\n",
      "Model  no_repeated_words ; rand_score =  0.147058823529\n",
      "Model  lemmatized ; rand_score =  0.128720836685\n",
      "Model  identity_analysis_2 ; rand_score =  0.119629317607\n",
      "Model  trigrams ; rand_score =  0.0877483443709\n",
      "Model  identity_analysis_1 ; rand_score =  0.0661016949153\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  0.0661016949153\n",
      "Model  primitive ; rand_score =  0.0562913907285\n",
      "Model  no_tomas_stemmed ; rand_score =  0.0235888795282\n",
      "Model  no_stop_words ; rand_score =  0.0\n",
      "Model  no_tomas_baker ; rand_score =  -0.00842459983151\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "CLASIFICATION WITH 10 CLUSTERS\n",
      "**************************************\n",
      "Getting results for the clustering mode  AgglomerativeClustering\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.522613065327\n",
      "Model  no_repeated_words ; rand_score =  0.445969125214\n",
      "Model  trigrams ; rand_score =  0.445969125214\n",
      "Model  identity_analysis_2 ; rand_score =  0.331658291457\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.222419928826\n",
      "Model  bigrams ; rand_score =  0.176834659593\n",
      "Model  lemmatized ; rand_score =  0.132497761862\n",
      "Model  identity_analysis_1 ; rand_score =  0.0650263620387\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  0.0434027777778\n",
      "Model  no_stop_words ; rand_score =  -0.0122270742358\n",
      "Model  no_tomas_stemmed ; rand_score =  -0.079012345679\n",
      "Model  stemmed_txts ; rand_score =  -0.079012345679\n",
      "Model  primitive ; rand_score =  -0.123230641132\n",
      "Model  no_tomas_baker ; rand_score =  -0.123230641132\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "**************************************\n",
      "Getting results for the clustering mode  MiniBatchKMeans\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.186899563319\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.175347222222\n",
      "Model  stemmed_txts ; rand_score =  0.175347222222\n",
      "Model  bigrams ; rand_score =  0.153711790393\n",
      "Model  lemmatized ; rand_score =  0.131147540984\n",
      "Model  no_tomas_stemmed ; rand_score =  0.0873362445415\n",
      "Model  primitive ; rand_score =  0.076726342711\n",
      "Model  no_tomas_baker ; rand_score =  0.076726342711\n",
      "Model  identity_analysis_2 ; rand_score =  0.0548885077187\n",
      "Model  no_repeated_words ; rand_score =  0.0452261306533\n",
      "Model  identity_analysis_1 ; rand_score =  0.02096069869\n",
      "Model  no_stop_words ; rand_score =  -0.0204603580563\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  -0.0204603580563\n",
      "Model  trigrams ; rand_score =  -0.0498054474708\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "**************************************\n",
      "Getting results for the clustering mode  KMeans\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.186899563319\n",
      "Model  identity_analysis_2 ; rand_score =  0.175347222222\n",
      "Model  lemmatized ; rand_score =  0.154804270463\n",
      "Model  bigrams ; rand_score =  0.109375\n",
      "Model  no_repeated_words ; rand_score =  0.0776699029126\n",
      "Model  trigrams ; rand_score =  0.0776699029126\n",
      "Model  no_tomas_stemmed ; rand_score =  0.076726342711\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.0452261306533\n",
      "Model  primitive ; rand_score =  0.0222984562607\n",
      "Model  identity_analysis_1 ; rand_score =  0.02096069869\n",
      "Model  no_tomas_baker ; rand_score =  0.0104166666667\n",
      "Model  no_stop_words ; rand_score =  0.0104166666667\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  0.0104166666667\n",
      "Model  stemmed_txts ; rand_score =  0.0104166666667\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "/////////////////////////////////\n",
      "CLUSTER ALGORITHM:  AgglomerativeClustering\n",
      "*Model: \" bigrams \". Best cluster agroupation:  8  clusters. Score:  0.303495311168\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  bigrams  is  0.202339986235\n",
      "*Model: \" no_tomas_stemmed \". Best cluster agroupation:  2  clusters. Score:  0.066762041011\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_tomas_stemmed  is  -0.0605700712589\n",
      "*Model: \" primitive \". Best cluster agroupation:  2  clusters. Score:  -0.0221852468109\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  primitive  is  -0.149642160052\n",
      "*Model: \" no_repeated_words \". Best cluster agroupation:  7  clusters. Score:  0.930606281958\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_repeated_words  is  0.33371040724\n",
      "*Model: \" identity_analysis_2 \". Best cluster agroupation:  9  clusters. Score:  0.434432823813\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  identity_analysis_2  is  0.0632368703108\n",
      "*Model: \" no_tomas_baker \". Best cluster agroupation:  2  clusters. Score:  -0.0221852468109\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_tomas_baker  is  -0.130111524164\n",
      "*Model: \" trigrams \". Best cluster agroupation:  6  clusters. Score:  0.785229841748\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  trigrams  is  0.493168510085\n",
      "*Model: \" named_ent_2_no_tomas_barker \". Best cluster agroupation:  6  clusters. Score:  0.798484848485\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  named_ent_2_no_tomas_barker  is  0.744623655914\n",
      "*Model: \" no_stop_words \". Best cluster agroupation:  9  clusters. Score:  0.0222984562607\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_stop_words  is  -0.10854816825\n",
      "*Model: \" lemmatized \". Best cluster agroupation:  5  clusters. Score:  0.226347305389\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  lemmatized  is  0.0632368703108\n",
      "*Model: \" identity_analysis_1 \". Best cluster agroupation:  4  clusters. Score:  0.161764705882\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  identity_analysis_1  is  0.161764705882\n",
      "*Model: \" no_tomas_stemmed_ent1 \". Best cluster agroupation:  5  clusters. Score:  0.816044260028\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_tomas_stemmed_ent1  is  0.744623655914\n",
      "*Model: \" no_tomas_stemmed_no_stop \". Best cluster agroupation:  2  clusters. Score:  0.066762041011\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_tomas_stemmed_no_stop  is  -0.0925\n",
      "*Model: \" stemmed_txts \". Best cluster agroupation:  2  clusters. Score:  0.066762041011\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  stemmed_txts  is  -0.0925\n",
      "/////////////////////////////////\n",
      "\n",
      "/////////////////////////////////\n",
      "CLUSTER ALGORITHM:  KMeans\n",
      "*Model: \" bigrams \". Best cluster agroupation:  9  clusters. Score:  0.2704\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  bigrams  is  -0.058949624866\n",
      "*Model: \" no_tomas_stemmed \". Best cluster agroupation:  7  clusters. Score:  0.1184\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_tomas_stemmed  is  0.0203527815468\n",
      "*Model: \" primitive \". Best cluster agroupation:  4  clusters. Score:  0.175033921303\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  primitive  is  0.175033921303\n",
      "*Model: \" no_repeated_words \". Best cluster agroupation:  5  clusters. Score:  0.249101796407\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_repeated_words  is  0.0836012861736\n",
      "*Model: \" identity_analysis_2 \". Best cluster agroupation:  6  clusters. Score:  0.336592178771\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  identity_analysis_2  is  0.230446927374\n",
      "*Model: \" no_tomas_baker \". Best cluster agroupation:  4  clusters. Score:  0.123473541384\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_tomas_baker  is  0.123473541384\n",
      "*Model: \" trigrams \". Best cluster agroupation:  8  clusters. Score:  0.228782287823\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  trigrams  is  -0.058949624866\n",
      "*Model: \" named_ent_2_no_tomas_barker \". Best cluster agroupation:  3  clusters. Score:  0.503267973856\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  named_ent_2_no_tomas_barker  is  0.0725593667546\n",
      "*Model: \" no_stop_words \". Best cluster agroupation:  4  clusters. Score:  0.149253731343\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_stop_words  is  0.149253731343\n",
      "*Model: \" lemmatized \". Best cluster agroupation:  6  clusters. Score:  0.564280750521\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  lemmatized  is  0.104949314252\n",
      "*Model: \" identity_analysis_1 \". Best cluster agroupation:  6  clusters. Score:  0.194579569145\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  identity_analysis_1  is  -0.0569877883311\n",
      "*Model: \" no_tomas_stemmed_ent1 \". Best cluster agroupation:  9  clusters. Score:  0.259322033898\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_tomas_stemmed_ent1  is  0.147039687703\n",
      "*Model: \" no_tomas_stemmed_no_stop \". Best cluster agroupation:  6  clusters. Score:  0.236669101534\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_tomas_stemmed_no_stop  is  0.175033921303\n",
      "*Model: \" stemmed_txts \". Best cluster agroupation:  9  clusters. Score:  0.150662251656\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  stemmed_txts  is  0.0203527815468\n",
      "/////////////////////////////////\n",
      "\n",
      "/////////////////////////////////\n",
      "CLUSTER ALGORITHM:  MiniBatchKMeans\n",
      "*Model: \" bigrams \". Best cluster agroupation:  10  clusters. Score:  0.153711790393\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  bigrams  is  -0.0830235439901\n",
      "*Model: \" no_tomas_stemmed \". Best cluster agroupation:  6  clusters. Score:  0.236669101534\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_tomas_stemmed  is  0.123473541384\n",
      "*Model: \" primitive \". Best cluster agroupation:  6  clusters. Score:  0.125639152666\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  primitive  is  0.123473541384\n",
      "*Model: \" no_repeated_words \". Best cluster agroupation:  6  clusters. Score:  0.564280750521\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_repeated_words  is  -0.0961538461538\n",
      "*Model: \" identity_analysis_2 \". Best cluster agroupation:  5  clusters. Score:  0.309593023256\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  identity_analysis_2  is  0.283519553073\n",
      "*Model: \" no_tomas_baker \". Best cluster agroupation:  6  clusters. Score:  0.236669101534\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_tomas_baker  is  0.123473541384\n",
      "*Model: \" trigrams \". Best cluster agroupation:  4  clusters. Score:  0.161764705882\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  trigrams  is  0.161764705882\n",
      "*Model: \" named_ent_2_no_tomas_barker \". Best cluster agroupation:  5  clusters. Score:  0.473614775726\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  named_ent_2_no_tomas_barker  is  0.283519553073\n",
      "*Model: \" no_stop_words \". Best cluster agroupation:  6  clusters. Score:  0.344827586207\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_stop_words  is  0.149253731343\n",
      "*Model: \" lemmatized \". Best cluster agroupation:  9  clusters. Score:  0.323529411765\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  lemmatized  is  -0.058949624866\n",
      "*Model: \" identity_analysis_1 \". Best cluster agroupation:  9  clusters. Score:  0.238704177323\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  identity_analysis_1  is  -0.0605700712589\n",
      "*Model: \" no_tomas_stemmed_ent1 \". Best cluster agroupation:  8  clusters. Score:  0.346972176759\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_tomas_stemmed_ent1  is  -0.0605700712589\n",
      "*Model: \" no_tomas_stemmed_no_stop \". Best cluster agroupation:  4  clusters. Score:  0.175033921303\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_tomas_stemmed_no_stop  is  0.175033921303\n",
      "*Model: \" stemmed_txts \". Best cluster agroupation:  9  clusters. Score:  0.247683235046\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  stemmed_txts  is  0.123473541384\n",
      "/////////////////////////////////\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tested_models = {}\n",
    "top_cluster = 10\n",
    "best_scores_all_clusters = {}\n",
    "best_scores_Realcluster = {}\n",
    "real_cluster_grouping = 4\n",
    "init_scores = -999999\n",
    "header_fields_compare = header_fields + ['comparing_mode']\n",
    "csv_file = 'CSV_output/rangeclusters.csv'\n",
    "\n",
    "with open(csv_file, 'a') as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerow(header_fields)\n",
    "    new_row = []\n",
    "\n",
    "    for cluster_mode in clustering_modes:\n",
    "        best_scores_all_clusters[cluster_mode] = {}\n",
    "        best_scores_Realcluster[cluster_mode] = {}\n",
    "\n",
    "    for cluster in range(1,top_cluster+1):\n",
    "        fix_grouping = cluster\n",
    "        print('CLASIFICATION WITH ' + str(fix_grouping) + ' CLUSTERS')\n",
    "        for cluster_mode in clustering_modes:\n",
    "            tested_models[cluster_mode] = {}        \n",
    "            tested_models[cluster_mode][\"primitive\"] = cluster_texts(raw_texts,fix_grouping,distanceFunction, cluster_mode)\n",
    "            tested_models[cluster_mode][\"identity_analysis_1\"] = cluster_texts(named_ent_txts_1,fix_grouping,distanceFunction, cluster_mode)\n",
    "            tested_models[cluster_mode][\"stemmed_txts\"] = cluster_texts(stemmed_txts,fix_grouping,distanceFunction, cluster_mode)\n",
    "            tested_models[cluster_mode][\"no_tomas_baker\"] = cluster_texts(texts_no_thomas_baker,fix_grouping,distanceFunction, cluster_mode)\n",
    "            tested_models[cluster_mode][\"no_stop_words\"] = cluster_texts(texts_no_stop_words,fix_grouping,distanceFunction, cluster_mode)\n",
    "            tested_models[cluster_mode][\"no_tomas_stemmed\"] = cluster_texts(no_tomas_stemmed_txts,fix_grouping,distanceFunction, cluster_mode)\n",
    "            tested_models[cluster_mode][\"no_tomas_stemmed_no_stop\"] = cluster_texts(no_tomas_stemmed_no_stop_txts,fix_grouping,\n",
    "                                                                                    distanceFunction, cluster_mode)\n",
    "            tested_models[cluster_mode][\"no_tomas_stemmed_ent1\"] = cluster_texts(no_tomas_stemmed_ent1_txts,fix_grouping,distanceFunction, \n",
    "                                                                                 cluster_mode)\n",
    "            tested_models[cluster_mode][\"identity_analysis_2\"] = cluster_texts(named_ent_txts_2,fix_grouping,distanceFunction, cluster_mode)\n",
    "            tested_models[cluster_mode][\"named_ent_2_no_tomas_barker\"] = cluster_texts(named_ent_2_no_tomas_barker_txts,\n",
    "                                                                     fix_grouping,distanceFunction, cluster_mode)\n",
    "            tested_models[cluster_mode]['bigrams'] = cluster_texts(bigrams_texts,fix_grouping,distanceFunction, cluster_mode)\n",
    "            tested_models[cluster_mode]['trigrams'] = cluster_texts(trigrams_texts, fix_grouping,distanceFunction, cluster_mode)\n",
    "            tested_models[cluster_mode]['lemmatized'] = cluster_texts(lemmatized_texts, fix_grouping,distanceFunction, cluster_mode)\n",
    "            tested_models[cluster_mode]['no_repeated_words'] = cluster_texts(no_repeatedWords_noStopWords,\n",
    "                                                                             fix_grouping, distanceFunction, cluster_mode)\n",
    "\n",
    "\n",
    "            tested_models[cluster_mode]['stanford_ner_txts'] = cluster_texts(stanford_ner_txts,fix_grouping, \n",
    "                                                                         distanceFunction, cluster_mode)\n",
    "            tested_models[cluster_mode]['stanford_ner_no_thomas_baker'] = cluster_texts(stanford_ner_no_thomas_baker,fix_grouping, \n",
    "                                                                         distanceFunction, cluster_mode)\n",
    "\n",
    "\n",
    "        # Evaluation\n",
    "        tested_models_scores = {}\n",
    "\n",
    "        for cluster_mode in tested_models:\n",
    "            tested_models_scores[cluster_mode] = {}        \n",
    "            for model in tested_models[cluster_mode]:            \n",
    "                tested_models_scores[cluster_mode][model] = adjusted_rand_score(reference,tested_models[cluster_mode][model])\n",
    "\n",
    "                # Calculate the max score for each model, each custer amount\n",
    "                if model in best_scores_all_clusters[cluster_mode].keys():\n",
    "                    if best_scores_all_clusters[cluster_mode][model]['score'] <= tested_models_scores[cluster_mode][model]:\n",
    "                        best_scores_all_clusters[cluster_mode][model]['cluster'] = cluster\n",
    "                        best_scores_all_clusters[cluster_mode][model]['score'] = tested_models_scores[cluster_mode][model] \n",
    "                else:\n",
    "                    best_scores_all_clusters[cluster_mode][model] = {}\n",
    "                    best_scores_all_clusters[cluster_mode][model]['cluster'] = cluster\n",
    "                    best_scores_all_clusters[cluster_mode][model]['score'] = init_scores    \n",
    "\n",
    "                if cluster == real_cluster_grouping:\n",
    "                    best_scores_Realcluster[cluster_mode][model] = {}\n",
    "                    best_scores_Realcluster[cluster_mode][model]['cluster'] = cluster\n",
    "                    best_scores_Realcluster[cluster_mode][model]['score'] = tested_models_scores[cluster_mode][model]              \n",
    "\n",
    "        for cluster_mode in tested_models_scores:\n",
    "            print(\"**************************************\")\n",
    "            print(\"Getting results for the clustering mode \", cluster_mode)\n",
    "            for model in sorted(tested_models_scores[cluster_mode].items(), key=operator.itemgetter(1), reverse=True):\n",
    "                print(\"Model \", model[0], \"; rand_score = \", model[1])            \n",
    "            print(\"########################################\")\n",
    "            print(\"########################################\\n\")\n",
    "\n",
    "            new_row = [cluster_mode, model[0], model[1], cluster]\n",
    "            writer.writerow(new_row)\n",
    "\n",
    "        \n",
    "csv_file = 'CSV_output/bestclusters.csv'\n",
    "with  open(csv_file, 'a') as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerow(header_fields_compare)\n",
    "    new_row = []\n",
    "    for cluster_mode in clustering_modes:\n",
    "        print('/////////////////////////////////')\n",
    "        print('CLUSTER ALGORITHM: ', cluster_mode)\n",
    "        for model in best_scores_all_clusters[cluster_mode].keys():\n",
    "\n",
    "            print('*Model: \"', model, '\". Best cluster agroupation: ',\n",
    "                      best_scores_all_clusters[cluster_mode][model]['cluster'],\n",
    "                      ' clusters. Score: ', \n",
    "                      str(best_scores_all_clusters[cluster_mode][model]['score'])) \n",
    "            new_row = [cluster_mode, model,\n",
    "                       best_scores_all_clusters[cluster_mode][model]['score'],\n",
    "                      best_scores_all_clusters[cluster_mode][model]['cluster'], 'best']\n",
    "            writer.writerow(new_row)        \n",
    "            \n",
    "            print('++Score  for the real cluster agroupation (',str(real_cluster_grouping),\n",
    "                  ') in model ', model, ' is ',\n",
    "                  str(best_scores_Realcluster[cluster_mode][model]['score'])) \n",
    "            new_row = [cluster_mode, model,\n",
    "                       str(best_scores_Realcluster[cluster_mode][model]['score']),\n",
    "                       str(real_cluster_grouping), 'real']\n",
    "            writer.writerow(new_row)    \n",
    "        print('/////////////////////////////////\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# En la memoria se ha de justificar cada cambio propuesto.\n",
    "\n",
    "# Se ha de comentar los resultados obtenidos.\n",
    "\n",
    "# Siempre buscando una mejora en la clusterización"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
