{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Práctica 1. Diferenciación de entidades.\n",
    "## Autores: \n",
    "     Raúl Sánchez Martín\n",
    "     Ignacio Arias Barra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re, pprint, os, numpy\n",
    "# -*- coding: utf-8 -*-\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "import goslate\n",
    "from nltk.collocations import *\n",
    "import string\n",
    "path_to_append = '/media/nacho/f8371289-0f00-4406-89db-d575f3cdb35e/Master/Trimestre 2/RIM/nltk_data'\n",
    "# path_to_append = '/media/raul/Data/nltk_data'\n",
    "path_to_append = '/home/raul/nltk_data'\n",
    "nltk.data.path.append(path_to_append)\n",
    "from sklearn.metrics.cluster import *\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans, MiniBatchKMeans\n",
    "from nltk.cluster import GAAClusterer\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from nltk.corpus import stopwords\n",
    "import operator\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from nltk.tag import StanfordNERTagger\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Código origen\n",
    "Lectura de fichero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    myfile = open(file,\"r\")\n",
    "    data = \"\"\n",
    "    lines = myfile.readlines()\n",
    "    for line in lines:\n",
    "        data = data + line\n",
    "    myfile.close\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cluster_texts(texts, clustersNumber, distanceFunction, clusterMode):\n",
    "    # Convierte textos en una coleccion\n",
    "    # Load the list of texts into a TextCollection object.\n",
    "    collection = nltk.TextCollection(texts)    \n",
    "#     print(\"Created a collection of\", len(collection), \"terms.\")\n",
    "    \n",
    "    # Get a list of unique terms\n",
    "    unique_terms = list(set(collection))\n",
    "#     print(\"Unique terms found: \", len(unique_terms))\n",
    "\n",
    "    ### And here we actually call the function and create our array of vectors.\n",
    "    # TF mide la frecuencia en los textos.\n",
    "    # Mira de los terminos unicos, cuantas veces aparece en el documento. No mira cuantas veces aparece en la coleccion\n",
    "    # Hay otras medidas, como TF-IDF que son mas precisas porque tambien miran cuantas veces aparece en la coleccion\n",
    "    vectors = [numpy.array(TF(f,unique_terms, collection)) for f in texts]\n",
    "    # print(\"Vectors created.\")\n",
    "    # print(vectors)\n",
    "    \n",
    "#     for vector in vectors:\n",
    "#         print(\"Vector \", len(vector))\n",
    "\n",
    "#     # initialize the clusterer\n",
    "#     clusterer = GAAClusterer(clustersNumber)\n",
    "#     clusters = clusterer.cluster(vectors, True)\n",
    "    # Estas lineas siguientes comentadas es lo mismo pero con otra libreria, la llamada scikit-learn\n",
    "    \n",
    "    if clusterMode == \"AgglomerativeClustering\":\n",
    "    \n",
    "        clusterer = AgglomerativeClustering(n_clusters=clustersNumber,\n",
    "                                         linkage=\"average\", affinity=distanceFunction)\n",
    "        clusters = clusterer.fit_predict(vectors)\n",
    "    \n",
    "    elif clusterMode == \"KMeans\":\n",
    "    \n",
    "        clusterer = KMeans(n_clusters=clustersNumber, random_state=0)\n",
    "        clusters = clusterer.fit(vectors).predict(vectors)\n",
    "        \n",
    "    elif clusterMode == \"MiniBatchKMeans\":\n",
    "        \n",
    "        clusterer = MiniBatchKMeans(n_clusters=clustersNumber, random_state=0)\n",
    "        clusters = clusterer.fit(vectors).predict(vectors)\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Frecuencia de términos únicos en un documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Function to create a TF vector for one document. For each of\n",
    "# our unique words, we have a feature which is the tf for that word\n",
    "# in the current document\n",
    "def TF(document, unique_terms, collection):\n",
    "    word_tf = []\n",
    "    for word in unique_terms:\n",
    "        word_tf.append(collection.tf(word, document))\n",
    "    return word_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Nuevo código\n",
    "\n",
    "*FUNCIÓN PARA DETECTAR EL IDIOMA DE UN TEXTO*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_language(possible_lan, text):\n",
    "    # More info in: http://blog.alejandronolla.com/2013/05/15/detecting-text-language-with-python-and-nltk/\n",
    "    languages_score = {}\n",
    "    for language in possible_lan:\n",
    "        stopwords_set = set(stopwords.words(language))\n",
    "        words_set = set(text)\n",
    "        common_elements = words_set.intersection(stopwords_set)\n",
    "        languages_score[possible_lan[language]] = len(common_elements)\n",
    "    return max(languages_score.items(), key=operator.itemgetter(1))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*USO DE ENTIDADES I*\n",
    "\n",
    "Se hará uso de las entidades reconocidas utilizando el método *ne_chunk* de la propia librería *nltk*. Simplemente se diferenciará entre nombres, adjetivos, adverbios, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_named_entities_1(initial_document, selected_types):\n",
    "    named_entities = list()\n",
    "    selected_entities = list()\n",
    "    try:\n",
    "        for sentence in initial_document:\n",
    "            tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "            # tagged_sentence = nltk.pos_tag(tokenized_sentence)\n",
    "            tagged_sentence = nltk.pos_tag(tokenized_sentence, tagset= 'universal')\n",
    "            named_ent = nltk.ne_chunk(tagged_sentence, binary=False)\n",
    "            named_entities.append(named_ent)\n",
    "\n",
    "        for element in named_entities:\n",
    "            word = element.pos()[0][0][0]\n",
    "            type_word = element.pos()[0][0][1]\n",
    "            if type_word in selected_types:\n",
    "                selected_entities.append(word)\n",
    "        \n",
    "        return selected_entities\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "def get_named_entities_2(initial_document):\n",
    "    named_entities = list()\n",
    "    selected_entities = list()\n",
    "    try:\n",
    "        for sentence in initial_document:\n",
    "            tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "            tagged_sentence = nltk.pos_tag(tokenized_sentence)\n",
    "#             tagged_sentence = nltk.pos_tag(tokenized_sentence, tagset= 'universal')\n",
    "            named_ent = nltk.ne_chunk(tagged_sentence, binary=False)\n",
    "            named_entities.append(named_ent)\n",
    "        \n",
    "        for element in named_entities:\n",
    "            if hasattr(element[0], 'label') and element[0].label:\n",
    "                selected_entities.append(element[0].leaves()[0][0])\n",
    "        \n",
    "        return selected_entities \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "        \n",
    "def get_named_ent_txts_1(raw_texts):\n",
    "    named_ent_txts_1 = []\n",
    "    for text in raw_texts:\n",
    "        curr_named_ent = get_named_entities_1(text, types_included_1)\n",
    "        text_to_append = nltk.Text(curr_named_ent)\n",
    "        named_ent_txts_1.append(text_to_append)\n",
    "    return named_ent_txts_1\n",
    "\n",
    "\n",
    "def get_named_ent_txts_2(raw_texts):\n",
    "    named_ent_txts_2 = []\n",
    "    for text in raw_texts:\n",
    "        curr_named_ent = get_named_entities_2(text)\n",
    "        text_to_append = nltk.Text(curr_named_ent)\n",
    "        named_ent_txts_2.append(text_to_append)\n",
    "    return named_ent_txts_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*USO DE ENTIDADES II (STANFORD)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_entities_standorf(sample, types_named_entities):\n",
    "    # Select the first classifier model\n",
    "    stanford_classifier = os.environ.get('STANFORD_MODELS').split(':')[2]\n",
    "\n",
    "    # Get the path for the StandorfNERTagger\n",
    "    stanford_ner_path = os.environ.get('CLASSPATH').split(':')[0]\n",
    "    \n",
    "    st = StanfordNERTagger(stanford_classifier, stanford_ner_path, encoding='utf-8')\n",
    "    result_named_entities = st.tag(sample.split())\n",
    "    filtered_named_entities = []\n",
    "\n",
    "    for item in result_named_entities:\n",
    "        word, entity = item\n",
    "        if entity in types_named_entities:\n",
    "#             filtered_named_entities.append((word, entity))\n",
    "            filtered_named_entities.append(word)\n",
    "        \n",
    "    return filtered_named_entities\n",
    "\n",
    "\n",
    "def get_named_ent_txts_3(raw_texts, types_named_entities):\n",
    "    named_ent_txts_3 = []\n",
    "    for text in raw_texts:\n",
    "        curr_named_ent = get_entities_standorf(text, types_named_entities)\n",
    "        text_to_append = nltk.Text(curr_named_ent)\n",
    "        named_ent_txts_3.append(text_to_append)\n",
    "    return named_ent_txts_3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*ELIMINAR PALABRAS VACÍAS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# # Abrimos el archivo.\n",
    "# with open('./Thomas_Baker/005.txt', 'r', encoding=\"utf8\") as f:\n",
    "# # with open('./Thomas_Baker/036.txt', 'r', encoding=\"utf8\") as f:\n",
    "#     text = f.read()\n",
    "# f.close()\n",
    "\n",
    "\n",
    "def delete_stopwords(text,lan='en'):\n",
    "\n",
    "    if lan == 'en':        \n",
    "        # Obtenemos las sentencias del texto.\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        # Obtenemos las \"stopwords\" del inglés.\n",
    "        stop = set(stopwords.words('english'))\n",
    "\n",
    "    elif lan == 'es':        \n",
    "        # Elegimos el tokenizador para el español de la NLTK.\n",
    "        spanish_tokenizer = nltk.data.load('tokenizers/punkt/spanish.pickle')\n",
    "        # Obtenemos las sentencias del texto.\n",
    "        sentences = spanish_tokenizer.tokenize(text)        \n",
    "\n",
    "        # Obtenemos las \"stopwords\" del español.\n",
    "        stop = set(stopwords.words('spanish'))\n",
    "      \n",
    "    print(\"Frases originales: \")\n",
    "    print(sentences)\n",
    "    \n",
    "    stopw = []\n",
    "    for i in stop:\n",
    "        stopw.append(i)\n",
    "    print(\"Las stop words en lan = \" + lan)\n",
    "    print(stopw)\n",
    "    print()\n",
    "\n",
    "    # Eliminamos las palabras que coinciden con alguna \"stopword\".\n",
    "    print(\"Frases procesadas sin stopwords: \")\n",
    "    for sentence in sentences:\n",
    "        non_stop_sentence = \"\"\n",
    "        for word in sentence.lower().split():\n",
    "            if word not in stop:\n",
    "                non_stop_sentence = non_stop_sentence + word + \" \"\n",
    "        # Imprimimos las sentencias procesadas.\n",
    "        print(non_stop_sentence)\n",
    "    return non_stop_sentence\n",
    "\n",
    "def delete_words_from_text(text, words_to_delete):\n",
    "    \n",
    "    words_to_include = []\n",
    "    words_to_delete = [word.lower() for word in words_to_delete]\n",
    "    \n",
    "    for word in text:\n",
    "        if word.lower() not in words_to_delete:\n",
    "            words_to_include.append(word)\n",
    "            \n",
    "    return words_to_include\n",
    "\n",
    "def get_texts_no_stop_words(raw_texts):\n",
    "    filtered_texts = []\n",
    "    for text in raw_texts:\n",
    "        language = {'en':'english', 'es':'spanish'}[get_language(possible_lan, text)]\n",
    "        words_to_exclude = list(set(stopwords.words(language)))\n",
    "        curr_filtered_text = delete_words_from_text(text, words_to_exclude)\n",
    "        filtered_texts.append(nltk.Text(curr_filtered_text))\n",
    "    return filtered_texts  \n",
    "\n",
    "# espanol = delete_stopwords(text,lan='es')\n",
    "# ingles = delete_stopwords(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*ELIMINAR EL NOMBRE DE LA PERSONA EN LOS TEXTOS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Eliminar las palabras ['Thomas', 'Baker'] de los textos\n",
    "\n",
    "def get_texts_exclude_tomas_baker(raw_texts):\n",
    "    filtered_texts = []\n",
    "    for text in raw_texts:\n",
    "        curr_filtered_text = delete_words_from_text(text, ['Thomas', 'Baker'])\n",
    "        filtered_texts.append(nltk.Text(curr_filtered_text))\n",
    "    return filtered_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*REPRESENTACIÓN DE NGRAMAS A NIVEL DE PALABRAS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Representación de ngramas\n",
    "\n",
    "def get_ngram(raw_texts, ngramlimit):\n",
    "    ngramlist=[]\n",
    "    for text in raw_texts:\n",
    "        ngram_text = nltk.ngrams(text, ngramlimit)\n",
    "        ngramlist.append(nltk.Text(ngram_text))\n",
    "    return ngramlist\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*STEAMMING*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# # Abrimos el archivo.\n",
    "# with open('./Thomas_Baker/005.txt', 'r', encoding=\"utf8\") as f:\n",
    "# with open('./Thomas_Baker/036.txt', 'r', encoding=\"utf8\") as f:\n",
    "#     text = f.read()\n",
    "# f.close()\n",
    "\n",
    "def stemming_original(text, lan='en'):\n",
    "    stemmeds = []\n",
    "    if lan == 'en':\n",
    "        # Steamer ingles\n",
    "        stemmer = PorterStemmer()        \n",
    "    elif lan == 'es':\n",
    "        # Stemer espanol        \n",
    "        stemmer = SnowballStemmer(\"spanish\")\n",
    "    \n",
    "    # Obtenemos los tokens del texto.\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "        \n",
    "    # Para cada token del texto obtenemos su raíz.\n",
    "    for token in tokens:\n",
    "        stemmed = stemmer.stem(token)\n",
    "        stemmeds.append(stemmed)\n",
    "    # Escribimos el resultado para compararlo con las palabras originales.\n",
    "    print(\"Palabras originales: \")\n",
    "    print(tokens)\n",
    "    print(\"Palabras procesadas con Snowball: \")\n",
    "    print(stemmeds)\n",
    "    \n",
    "    \n",
    "def stemming(text, lan='en'):\n",
    "    stemmeds = []\n",
    "    if lan == 'en':\n",
    "        # Steamer ingles\n",
    "        stemmer = PorterStemmer()        \n",
    "    elif lan == 'es':\n",
    "        # Stemer espanol        \n",
    "        stemmer = SnowballStemmer(\"spanish\")\n",
    "    \n",
    "    # Obtenemos los tokens del texto.\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "        \n",
    "    # Para cada token del texto obtenemos su raíz.\n",
    "    for token in tokens:\n",
    "        stemmed = stemmer.stem(token)\n",
    "        stemmeds.append(stemmed)\n",
    "    # Escribimos el resultado para compararlo con las palabras originales.\n",
    "    return stemmeds\n",
    "\n",
    "\n",
    "possible_lan = {\"english\":\"en\", \"spanish\":\"es\"}\n",
    "def get_stemmed_txts(raw_texts):\n",
    "    stemmed_txts = []\n",
    "    for text in raw_texts:\n",
    "        language = get_language(possible_lan, text)\n",
    "        stemmed_txt = stemming(text, language)\n",
    "        text = nltk.Text(stemmed_txt)\n",
    "        stemmed_txts.append(text)\n",
    "    return stemmed_txts\n",
    "\n",
    "def stemming_2(text, lan='en'):\n",
    "    stemmeds = []\n",
    "    if lan == 'en':\n",
    "        # Steamer ingles\n",
    "        stemmer = PorterStemmer()        \n",
    "    elif lan == 'es':\n",
    "        # Stemer espanol        \n",
    "        stemmer = SnowballStemmer(\"spanish\")\n",
    "        \n",
    "    # Para cada token del texto obtenemos su raíz.\n",
    "    for word in text:\n",
    "        stemmed = stemmer.stem(word)\n",
    "        stemmeds.append(stemmed)\n",
    "    # Escribimos el resultado para compararlo con las palabras originales.\n",
    "    return stemmeds\n",
    "\n",
    "def get_stemmed_txts_2(raw_texts):\n",
    "    stemmed_txts = []\n",
    "    for text in raw_texts:\n",
    "        language = get_language(possible_lan, text)\n",
    "        stemmed_txt = stemming_2(text, language)\n",
    "        text = nltk.Text(stemmed_txt)\n",
    "        stemmed_txts.append(text)\n",
    "    return stemmed_txts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*ELIMINAR PALABRAS REPETIDAS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Pasamos el texto a minúsculas.\n",
    "# Es necesario para no encontrar palabras repetidas que son la misma.\n",
    "def delete_repWords_stopWords(raw_texts):    \n",
    "    deleted_repeated = []\n",
    "    for text in raw_texts:\n",
    "        delete_repeated_texts = []\n",
    "        text = [w.lower() for w in text]  \n",
    "        unique_terms = list(set(text))\n",
    "#         print(\"Número de palabras del texto: \",str(len(text)))\n",
    "#         print(\"Tamaño del vocabulario filtrado: \", str(len(unique_terms)))\n",
    "        deleted_repeated.append(nltk.Text(unique_terms))    \n",
    "    return deleted_repeated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*LEMATIZAR LOS TEXTOS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class SpanishLemmatizer():\n",
    "    # Abrimos el fichero donde tenemos la información para cada palabra y lo cargamos en un diccionario.\n",
    "    def __init__(self):\n",
    "        with open('./Lemmatizer/lemmatization-es.txt', 'r', encoding=\"utf8\") as f:\n",
    "            self.lemma_dict = {}\n",
    "            for line in f:\n",
    "                if line.strip(): # Evitamos posibles líneas en blanco.\n",
    "                    value, key = line.split(None, 1) # Nos quedamos con los valores clave y valor.\n",
    "                                                     # None implica espacio en blanco.\n",
    "                    key = key.rstrip() # Limpiamos la línea para evitar los caracteres de salto \\n ó \\r.\n",
    "                    self.lemma_dict[key] = value\n",
    "                    self.lemma_dict[value] = value  # Añadimos por si acaso también el valor como clave.\n",
    "\n",
    "    # Obtenemos el lemma para la palabra solicitada si es que se dispone de él. En caso contrario devuelve la palabra.\n",
    "    # Útil en los casos en los que no se haya aplicado un tratamiento previo del texto (stopwords y puntuación).\n",
    "    def lemmatize(self,word):\n",
    "        try:\n",
    "            lemma = self.lemma_dict[word.lower()]\n",
    "        except KeyError:\n",
    "            lemma = word\n",
    "        return lemma\n",
    "\n",
    "def lemmatize_texts(raw_texts, possible_lan):\n",
    "    nlemmas_texts = []\n",
    "    for text in raw_texts:\n",
    "        language = get_language(possible_lan, text)\n",
    "        if language== 'en':\n",
    "            # Seleccionamos el lematizador.\n",
    "            wordnet_lemmatizer = WordNetLemmatizer()\n",
    "            # Obtenemos los tokens de las sentencias.\n",
    "#             tokens = nltk.word_tokenize(text)\n",
    "            \n",
    "            lemmatizeds = []\n",
    "            nlemmas = []\n",
    "\n",
    "            for token in text:                \n",
    "                lemmatized = wordnet_lemmatizer.lemmatize(token)\n",
    "                lemmatizeds.append(lemmatized)\n",
    "                #print('token ',token)\n",
    "                #print('lema ',lemmatized)\n",
    "                # Obtenemos los lemmas consultando la base de datos de WordNet.\n",
    "                list = wordnet.synsets(token)\n",
    "                # Si encontramos alguna palabra relacionada obtenemos sus lemas y nos quedamos con el primero.\n",
    "                if len(list) >= 1:\n",
    "                    lemma = list[0].lemma_names('eng')\n",
    "                    if len(lemma) > 1:\n",
    "                        nlemmas.append(lemma[0])\n",
    "                    else:\n",
    "                        nlemmas.append((token))\n",
    "                # En caso contrario simplemente introducimos en la solución la palabra actual.\n",
    "            else:\n",
    "                nlemmas.append(token)\n",
    "            nlemmas_texts.append(nltk.Text(nlemmas))\n",
    "        elif language== 'es':\n",
    "            lemmatizer = SpanishLemmatizer()\n",
    "\n",
    "            # Obtenemos los tokens del texto.\n",
    "#             tokens = nltk.word_tokenize(text)            \n",
    "            nlemmas = []\n",
    "            lemmatizeds = []\n",
    "            for token in text:                \n",
    "                # Obtenemos los lemmas consultando el archivo de lemmas.\n",
    "                lemmatized = lemmatizer.lemmatize(token)\n",
    "                #print('token ',token)\n",
    "                #print('lema ',lemmatized)\n",
    "                lemmatizeds.append(lemmatized)\n",
    "                # Obtenemos los lemmas consultando la base de datos de WordNet.\n",
    "                list = wordnet.synsets(token, lang='spa')\n",
    "                # Si encontramos alguna palabra relacionada obtenemos sus lemas y nos quedamos con el primero.\n",
    "                if len(list) >= 1:\n",
    "                    lemma = list[0].lemma_names('spa')\n",
    "                    if len(lemma) >= 1:\n",
    "                        nlemmas.append(lemma[0])\n",
    "                    else:\n",
    "                        nlemmas.append(token)\n",
    "                # En caso contrario simplemente introducimos en la solución la palabra actual.\n",
    "                else:\n",
    "                    nlemmas.append(token)\n",
    "            nlemmas_texts.append(nltk.Text(nlemmas))\n",
    "        else:\n",
    "            print('Lenguaje no reconocido')\n",
    "    return nlemmas_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Ideas del propio enunciado de la práctica (muchas incluidas ya antes pero simmplemente para comprobar):\n",
    "\n",
    "* Diferentes algoritmos de clustering\n",
    "* Eliminar palabras vacías\n",
    "* Stemming\n",
    "* Lematización\n",
    "* Reconocer entidades nombradas\n",
    "* Eliminar palabras repetidas\n",
    "* n-gramas\n",
    "* Probar diferentes números de clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Inicio programa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001.txt\n",
      "002.txt\n",
      "005.txt\n",
      "008.txt\n",
      "009.txt\n",
      "011.txt\n",
      "015.txt\n",
      "017.txt\n",
      "020.txt\n",
      "024.txt\n",
      "027.txt\n",
      "028.txt\n",
      "036.txt\n",
      "041.txt\n",
      "047.txt\n",
      "050.txt\n",
      "056.txt\n",
      "072.txt\n",
      "075.txt\n",
      "Prepared  19  documents...\n",
      "They can be accessed using texts[0] - texts[18]\n"
     ]
    }
   ],
   "source": [
    "# Folder with all texts\n",
    "folder = \"Thomas_Baker\"\n",
    "types_named_entities = [\"LOCATION\", \"PERSON\", \"ORGANIZATION\"]\n",
    "# Empty list to hold text documents.\n",
    "raw_texts = []\n",
    "raw_texts_2 = []\n",
    "named_ent_txts_1 = []\n",
    "types_included_1 = ['NOUN']\n",
    "\n",
    "possible_lan = {\"english\":\"en\", \"spanish\":\"es\"}\n",
    "clustering_modes = [\"AgglomerativeClustering\", \"KMeans\", \"MiniBatchKMeans\"]\n",
    "\n",
    "listing = os.listdir(folder)\n",
    "for file in sorted(listing):\n",
    "    if file.endswith(\".txt\"):\n",
    "        url = folder+\"/\"+file\n",
    "        print(file)\n",
    "        f = open(url,encoding=\"latin-1\");\n",
    "        raw = f.read()\n",
    "        f.close()\n",
    "        f2 = open(url, 'r', encoding=\"utf8\")\n",
    "        raw2 = f2.read()\n",
    "        f2.close()\n",
    "        raw_texts_2.append(raw2)\n",
    "        tokens = nltk.word_tokenize(raw)\n",
    "        text = nltk.Text(tokens)\n",
    "        raw_texts.append(text)\n",
    "        \n",
    "\n",
    "\n",
    "print(\"Prepared \", len(raw_texts), \" documents...\")\n",
    "print(\"They can be accessed using texts[0] - texts[\" + str(len(raw_texts)-1) + \"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Stanford NER...\n",
      "Using Stanford NER removing Thomas Baker...\n",
      "Removing non-stop words....\n",
      "Removing Thomas Baker from the texts...\n",
      "Getting text including only named entities according to criteria 1...\n",
      "Getting stemmed texts...\n",
      "Getting no_tomas_stemmed...\n",
      "Getting no_tomas_stemmed_no_stop...\n",
      "Getting no_tomas_stemmed_ent1...\n",
      "Getting text including only named entities according to criteria 2...\n",
      "Getting text including only named entities according to criteria 2 and excluding the words 'Tomas' and 'Baker'\n",
      "Getting bigrams in texts\n",
      "Getting trigrams in texts\n",
      "Lemmatizing texts\n",
      "Removing repeated words\n",
      "reference:  [0, 1, 2, 0, 0, 0, 3, 0, 0, 0, 2, 0, 3, 3, 0, 1, 2, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print('Using Stanford NER...')\n",
    "stanford_ner_txts = get_named_ent_txts_3(raw_texts_2, types_named_entities)\n",
    "\n",
    "print('Using Stanford NER removing Thomas Baker...')\n",
    "stanford_ner_no_thomas_baker = get_texts_exclude_tomas_baker(stanford_ner_txts)\n",
    "\n",
    "print(\"Removing non-stop words....\")\n",
    "texts_no_stop_words = get_texts_no_stop_words(raw_texts)\n",
    "\n",
    "print(\"Removing Thomas Baker from the texts...\")\n",
    "texts_no_thomas_baker = get_texts_exclude_tomas_baker(raw_texts)\n",
    "\n",
    "print(\"Getting text including only named entities according to criteria 1...\")\n",
    "named_ent_txts_1 = get_named_ent_txts_1(raw_texts)\n",
    "\n",
    "print(\"Getting stemmed texts...\")\n",
    "stemmed_txts = get_stemmed_txts_2(raw_texts)\n",
    "\n",
    "print(\"Getting no_tomas_stemmed...\")\n",
    "no_tomas_stemmed_txts = get_stemmed_txts_2(texts_no_thomas_baker)\n",
    "\n",
    "print(\"Getting no_tomas_stemmed_no_stop...\")\n",
    "no_tomas_stemmed_no_stop_txts = get_texts_no_stop_words(no_tomas_stemmed_txts)\n",
    "\n",
    "print(\"Getting no_tomas_stemmed_ent1...\")\n",
    "no_tomas_stemmed_ent1_txts = get_named_ent_txts_1(no_tomas_stemmed_txts)\n",
    "\n",
    "print(\"Getting text including only named entities according to criteria 2...\")\n",
    "named_ent_txts_2 = get_named_ent_txts_2(raw_texts)\n",
    "\n",
    "print(\"Getting text including only named entities according to criteria 2 and excluding the words 'Tomas' and 'Baker'\")\n",
    "named_ent_2_no_tomas_barker_txts = get_texts_exclude_tomas_baker(named_ent_txts_2)\n",
    "\n",
    "print('Getting bigrams in texts')\n",
    "bigrams_texts = get_ngram(raw_texts, 2)\n",
    "\n",
    "print('Getting trigrams in texts')\n",
    "trigrams_texts = get_ngram(raw_texts, 3)\n",
    "\n",
    "print('Lemmatizing texts')\n",
    "lemmatized_texts = lemmatize_texts(raw_texts, possible_lan)\n",
    "\n",
    "print('Removing repeated words')\n",
    "no_repeatedWords_noStopWords = delete_repWords_stopWords(raw_texts)\n",
    "\n",
    "\n",
    "    \n",
    "# named_ent_txts_2 = get_named_entities_2(raw_texts[0])\n",
    "# for item in named_ent_txts_2:\n",
    "#     if hasattr(item[0], 'label') and item[0].label:\n",
    "#         print(item[0].leaves()[0][0])\n",
    "\n",
    "# Similarity distance\n",
    "distanceFunction =\"cosine\"\n",
    "# distanceFunction = \"euclidean\"\n",
    "\n",
    "reference =[0, 1, 2, 0, 0, 0, 3, 0, 0, 0, 2, 0, 3, 3, 0, 1, 2, 0, 1]\n",
    "print(\"reference: \", reference)\n",
    "\n",
    "#------------------------------\n",
    "\n",
    "# text_1 = \"A football player is Raul Gonzalez\"\n",
    "# text_2 = \"Raul Gonzalez is a football player\"\n",
    "# text_3 = \"Data science is the future\"\n",
    "# text_4 = \"The future is data science\"\n",
    "\n",
    "# text_1 = nltk.Text(nltk.word_tokenize(text_1))\n",
    "# text_2 = nltk.Text(nltk.word_tokenize(text_2))\n",
    "# text_3 = nltk.Text(nltk.word_tokenize(text_3))\n",
    "# text_4 = nltk.Text(nltk.word_tokenize(text_4))\n",
    "\n",
    "\n",
    "\n",
    "# new_texts = [text_1, text_2, text_3, text_4]\n",
    "\n",
    "# collection_new = nltk.TextCollection(new_texts)\n",
    "\n",
    "# print(\"Number of times Raul appears \", collection_new.tf(\"Raul\", text_1))\n",
    "\n",
    "# texts_new = [text_1, text_2, text_3, text_4]\n",
    "# new_reference = [0, 0, 1, 1]\n",
    "# new_test = cluster_texts(texts_new, 2, distanceFunction)\n",
    "# print(\"New test \", new_test)\n",
    "# print(\"New rand score \", adjusted_rand_score(new_reference, new_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*CLUSTER FIJO CON 4 GRUPOS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************\n",
      "Getting results for the clustering mode  KMeans\n",
      "Model  identity_analysis_2 ; rand_score =  0.2304469273743017\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  0.17503392130257805\n",
      "Model  primitive ; rand_score =  0.17503392130257805\n",
      "Model  no_stop_words ; rand_score =  0.1492537313432836\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.14703968770331813\n",
      "Model  no_tomas_baker ; rand_score =  0.12347354138398917\n",
      "Model  lemmatized ; rand_score =  0.10494931425163982\n",
      "Model  no_repeated_words ; rand_score =  0.0836012861736335\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.07255936675461744\n",
      "Model  stanford_ner_no_thomas_baker ; rand_score =  0.05510752688172045\n",
      "Model  stanford_ner_txts ; rand_score =  0.051395007342143924\n",
      "Model  no_tomas_stemmed ; rand_score =  0.020352781546811426\n",
      "Model  stemmed_txts ; rand_score =  0.020352781546811426\n",
      "Model  identity_analysis_1 ; rand_score =  -0.05698778833107188\n",
      "Model  trigrams ; rand_score =  -0.058949624866023516\n",
      "Model  bigrams ; rand_score =  -0.058949624866023516\n",
      "########################################\n",
      "########################################\n",
      "**************************************\n",
      "Getting results for the clustering mode  MiniBatchKMeans\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.2835195530726257\n",
      "Model  identity_analysis_2 ; rand_score =  0.2835195530726257\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  0.17503392130257805\n",
      "Model  trigrams ; rand_score =  0.16176470588235295\n",
      "Model  no_stop_words ; rand_score =  0.1492537313432836\n",
      "Model  no_tomas_stemmed ; rand_score =  0.12347354138398917\n",
      "Model  stemmed_txts ; rand_score =  0.12347354138398917\n",
      "Model  primitive ; rand_score =  0.12347354138398917\n",
      "Model  no_tomas_baker ; rand_score =  0.12347354138398917\n",
      "Model  stanford_ner_no_thomas_baker ; rand_score =  0.07374999999999998\n",
      "Model  stanford_ner_txts ; rand_score =  0.051395007342143924\n",
      "Model  lemmatized ; rand_score =  -0.058949624866023516\n",
      "Model  identity_analysis_1 ; rand_score =  -0.060570071258907406\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  -0.060570071258907406\n",
      "Model  bigrams ; rand_score =  -0.08302354399008675\n",
      "Model  no_repeated_words ; rand_score =  -0.09615384615384613\n",
      "########################################\n",
      "########################################\n",
      "**************************************\n",
      "Getting results for the clustering mode  AgglomerativeClustering\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.7446236559139784\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.7446236559139784\n",
      "Model  trigrams ; rand_score =  0.49316851008458035\n",
      "Model  stanford_ner_no_thomas_baker ; rand_score =  0.4809976247030879\n",
      "Model  stanford_ner_txts ; rand_score =  0.34076827757125155\n",
      "Model  no_repeated_words ; rand_score =  0.33371040723981904\n",
      "Model  bigrams ; rand_score =  0.20233998623537508\n",
      "Model  identity_analysis_1 ; rand_score =  0.16176470588235295\n",
      "Model  lemmatized ; rand_score =  0.06323687031082535\n",
      "Model  identity_analysis_2 ; rand_score =  0.06323687031082535\n",
      "Model  no_tomas_stemmed ; rand_score =  -0.060570071258907406\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  -0.09250000000000001\n",
      "Model  stemmed_txts ; rand_score =  -0.09250000000000001\n",
      "Model  no_stop_words ; rand_score =  -0.10854816824966075\n",
      "Model  no_tomas_baker ; rand_score =  -0.1301115241635688\n",
      "Model  primitive ; rand_score =  -0.1496421600520495\n",
      "########################################\n",
      "########################################\n"
     ]
    }
   ],
   "source": [
    "tested_models = {}\n",
    "fix_grouping = 4\n",
    "for cluster_mode in clustering_modes:\n",
    "    tested_models[cluster_mode] = {}\n",
    "    tested_models[cluster_mode][\"primitive\"] = cluster_texts(raw_texts,fix_grouping,distanceFunction, cluster_mode)\n",
    "    tested_models[cluster_mode][\"identity_analysis_1\"] = cluster_texts(named_ent_txts_1,fix_grouping,distanceFunction, cluster_mode)\n",
    "    tested_models[cluster_mode][\"stemmed_txts\"] = cluster_texts(stemmed_txts,fix_grouping,distanceFunction, cluster_mode)\n",
    "    tested_models[cluster_mode][\"no_tomas_baker\"] = cluster_texts(texts_no_thomas_baker,fix_grouping,distanceFunction, cluster_mode)\n",
    "    tested_models[cluster_mode][\"no_stop_words\"] = cluster_texts(texts_no_stop_words,fix_grouping,distanceFunction, cluster_mode)\n",
    "    tested_models[cluster_mode][\"no_tomas_stemmed\"] = cluster_texts(no_tomas_stemmed_txts,fix_grouping,distanceFunction, cluster_mode)\n",
    "    tested_models[cluster_mode][\"no_tomas_stemmed_no_stop\"] = cluster_texts(no_tomas_stemmed_no_stop_txts,fix_grouping,\n",
    "                                                                            distanceFunction, cluster_mode)\n",
    "    tested_models[cluster_mode][\"no_tomas_stemmed_ent1\"] = cluster_texts(no_tomas_stemmed_ent1_txts,fix_grouping,distanceFunction, \n",
    "                                                                         cluster_mode)\n",
    "    tested_models[cluster_mode][\"identity_analysis_2\"] = cluster_texts(named_ent_txts_2,fix_grouping,distanceFunction, cluster_mode)\n",
    "    tested_models[cluster_mode][\"named_ent_2_no_tomas_barker\"] = cluster_texts(named_ent_2_no_tomas_barker_txts,\n",
    "                                                             fix_grouping,distanceFunction, cluster_mode)\n",
    "    tested_models[cluster_mode]['bigrams'] = cluster_texts(bigrams_texts,fix_grouping,distanceFunction, cluster_mode)\n",
    "    tested_models[cluster_mode]['trigrams'] = cluster_texts(trigrams_texts, fix_grouping,distanceFunction, cluster_mode)\n",
    "    tested_models[cluster_mode]['lemmatized'] = cluster_texts(lemmatized_texts, fix_grouping,distanceFunction, cluster_mode)\n",
    "    tested_models[cluster_mode]['no_repeated_words'] = cluster_texts(no_repeatedWords_noStopWords,fix_grouping, distanceFunction, cluster_mode)\n",
    "    tested_models[cluster_mode]['stanford_ner_txts'] = cluster_texts(stanford_ner_txts,fix_grouping, \n",
    "                                                                     distanceFunction, cluster_mode)\n",
    "    tested_models[cluster_mode]['stanford_ner_no_thomas_baker'] = cluster_texts(stanford_ner_no_thomas_baker,fix_grouping, \n",
    "                                                                     distanceFunction, cluster_mode)\n",
    "    \n",
    "\n",
    "# Evaluation\n",
    "tested_models_scores = {}\n",
    "\n",
    "for cluster_mode in tested_models:\n",
    "    tested_models_scores[cluster_mode] = {}\n",
    "    for model in tested_models[cluster_mode]:\n",
    "        tested_models_scores[cluster_mode][model] = adjusted_rand_score(reference,tested_models[cluster_mode][model])\n",
    "#     print(\"Model \", model, \"; rand_score = \", adjusted_rand_score(reference,tested_models[model]))\n",
    "    \n",
    "for cluster_mode in tested_models_scores:\n",
    "    print(\"**************************************\")\n",
    "    print(\"Getting results for the clustering mode \", cluster_mode)\n",
    "    for model in sorted(tested_models_scores[cluster_mode].items(), key=operator.itemgetter(1), reverse=True):\n",
    "        print(\"Model \", model[0], \"; rand_score = \", model[1])\n",
    "    print(\"########################################\")\n",
    "    print(\"########################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*DIFERENTE NÚMERO DE CLUSTERS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASIFICATION WITH 1 CLUSTERS\n",
      "**************************************\n",
      "Getting results for the clustering mode  KMeans\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  0.0\n",
      "Model  identity_analysis_1 ; rand_score =  0.0\n",
      "Model  no_tomas_stemmed ; rand_score =  0.0\n",
      "Model  stemmed_txts ; rand_score =  0.0\n",
      "Model  primitive ; rand_score =  0.0\n",
      "Model  trigrams ; rand_score =  0.0\n",
      "Model  bigrams ; rand_score =  0.0\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.0\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.0\n",
      "Model  lemmatized ; rand_score =  0.0\n",
      "Model  no_stop_words ; rand_score =  0.0\n",
      "Model  no_tomas_baker ; rand_score =  0.0\n",
      "Model  stanford_ner_no_thomas_baker ; rand_score =  0.0\n",
      "Model  identity_analysis_2 ; rand_score =  0.0\n",
      "Model  no_repeated_words ; rand_score =  0.0\n",
      "Model  stanford_ner_txts ; rand_score =  0.0\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "**************************************\n",
      "Getting results for the clustering mode  MiniBatchKMeans\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  0.0\n",
      "Model  identity_analysis_1 ; rand_score =  0.0\n",
      "Model  no_tomas_stemmed ; rand_score =  0.0\n",
      "Model  stemmed_txts ; rand_score =  0.0\n",
      "Model  primitive ; rand_score =  0.0\n",
      "Model  trigrams ; rand_score =  0.0\n",
      "Model  bigrams ; rand_score =  0.0\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.0\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.0\n",
      "Model  lemmatized ; rand_score =  0.0\n",
      "Model  no_stop_words ; rand_score =  0.0\n",
      "Model  no_tomas_baker ; rand_score =  0.0\n",
      "Model  stanford_ner_no_thomas_baker ; rand_score =  0.0\n",
      "Model  identity_analysis_2 ; rand_score =  0.0\n",
      "Model  no_repeated_words ; rand_score =  0.0\n",
      "Model  stanford_ner_txts ; rand_score =  0.0\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "**************************************\n",
      "Getting results for the clustering mode  AgglomerativeClustering\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  0.0\n",
      "Model  identity_analysis_1 ; rand_score =  0.0\n",
      "Model  no_tomas_stemmed ; rand_score =  0.0\n",
      "Model  stemmed_txts ; rand_score =  0.0\n",
      "Model  primitive ; rand_score =  0.0\n",
      "Model  trigrams ; rand_score =  0.0\n",
      "Model  bigrams ; rand_score =  0.0\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.0\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.0\n",
      "Model  lemmatized ; rand_score =  0.0\n",
      "Model  no_stop_words ; rand_score =  0.0\n",
      "Model  no_tomas_baker ; rand_score =  0.0\n",
      "Model  stanford_ner_no_thomas_baker ; rand_score =  0.0\n",
      "Model  identity_analysis_2 ; rand_score =  0.0\n",
      "Model  no_repeated_words ; rand_score =  0.0\n",
      "Model  stanford_ner_txts ; rand_score =  0.0\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "CLASIFICATION WITH 2 CLUSTERS\n",
      "**************************************\n",
      "Getting results for the clustering mode  KMeans\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  0.06676204101096801\n",
      "Model  identity_analysis_1 ; rand_score =  0.06676204101096801\n",
      "Model  no_tomas_stemmed ; rand_score =  0.06676204101096801\n",
      "Model  stemmed_txts ; rand_score =  0.06676204101096801\n",
      "Model  primitive ; rand_score =  0.06676204101096801\n",
      "Model  trigrams ; rand_score =  0.06676204101096801\n",
      "Model  bigrams ; rand_score =  0.06676204101096801\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.06676204101096801\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.06676204101096801\n",
      "Model  lemmatized ; rand_score =  0.06676204101096801\n",
      "Model  no_stop_words ; rand_score =  0.06676204101096801\n",
      "Model  no_tomas_baker ; rand_score =  0.06676204101096801\n",
      "Model  identity_analysis_2 ; rand_score =  0.06676204101096801\n",
      "Model  no_repeated_words ; rand_score =  0.06676204101096801\n",
      "Model  stanford_ner_txts ; rand_score =  -0.028624766645924053\n",
      "Model  stanford_ner_no_thomas_baker ; rand_score =  -0.11764705882352947\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "**************************************\n",
      "Getting results for the clustering mode  MiniBatchKMeans\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  0.06676204101096801\n",
      "Model  identity_analysis_1 ; rand_score =  0.06676204101096801\n",
      "Model  no_tomas_stemmed ; rand_score =  0.06676204101096801\n",
      "Model  stemmed_txts ; rand_score =  0.06676204101096801\n",
      "Model  primitive ; rand_score =  0.06676204101096801\n",
      "Model  trigrams ; rand_score =  0.06676204101096801\n",
      "Model  bigrams ; rand_score =  0.06676204101096801\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.06676204101096801\n",
      "Model  lemmatized ; rand_score =  0.06676204101096801\n",
      "Model  no_stop_words ; rand_score =  0.06676204101096801\n",
      "Model  no_tomas_baker ; rand_score =  0.06676204101096801\n",
      "Model  no_repeated_words ; rand_score =  0.06676204101096801\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.014311270125223617\n",
      "Model  stanford_ner_txts ; rand_score =  -0.028624766645924053\n",
      "Model  identity_analysis_2 ; rand_score =  -0.05723370429252788\n",
      "Model  stanford_ner_no_thomas_baker ; rand_score =  -0.11764705882352947\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "**************************************\n",
      "Getting results for the clustering mode  AgglomerativeClustering\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  0.06676204101096801\n",
      "Model  no_tomas_stemmed ; rand_score =  0.06676204101096801\n",
      "Model  stemmed_txts ; rand_score =  0.06676204101096801\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.06676204101096801\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.06676204101096801\n",
      "Model  lemmatized ; rand_score =  0.06676204101096801\n",
      "Model  stanford_ner_no_thomas_baker ; rand_score =  0.06676204101096801\n",
      "Model  identity_analysis_2 ; rand_score =  0.06676204101096801\n",
      "Model  no_repeated_words ; rand_score =  0.06676204101096801\n",
      "Model  stanford_ner_txts ; rand_score =  0.06676204101096801\n",
      "Model  identity_analysis_1 ; rand_score =  -0.005037783375314868\n",
      "Model  primitive ; rand_score =  -0.022185246810870803\n",
      "Model  no_stop_words ; rand_score =  -0.022185246810870803\n",
      "Model  no_tomas_baker ; rand_score =  -0.022185246810870803\n",
      "Model  trigrams ; rand_score =  -0.06008583690987128\n",
      "Model  bigrams ; rand_score =  -0.06008583690987128\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "CLASIFICATION WITH 3 CLUSTERS\n",
      "**************************************\n",
      "Getting results for the clustering mode  KMeans\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.5032679738562091\n",
      "Model  lemmatized ; rand_score =  0.23918846769887883\n",
      "Model  stemmed_txts ; rand_score =  0.14703968770331813\n",
      "Model  stanford_ner_txts ; rand_score =  0.05673758865248223\n",
      "Model  stanford_ner_no_thomas_baker ; rand_score =  0.02247191011235955\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  0.001011122345803871\n",
      "Model  no_tomas_stemmed ; rand_score =  0.001011122345803871\n",
      "Model  primitive ; rand_score =  0.001011122345803871\n",
      "Model  trigrams ; rand_score =  0.001011122345803871\n",
      "Model  bigrams ; rand_score =  0.001011122345803871\n",
      "Model  no_tomas_baker ; rand_score =  0.001011122345803871\n",
      "Model  no_repeated_words ; rand_score =  0.001011122345803871\n",
      "Model  identity_analysis_1 ; rand_score =  -0.03636363636363637\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  -0.03636363636363637\n",
      "Model  no_stop_words ; rand_score =  -0.044847837693539755\n",
      "Model  identity_analysis_2 ; rand_score =  -0.044847837693539755\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "**************************************\n",
      "Getting results for the clustering mode  MiniBatchKMeans\n",
      "Model  stanford_ner_txts ; rand_score =  0.15814587593728696\n",
      "Model  identity_analysis_2 ; rand_score =  0.12570145903479232\n",
      "Model  stanford_ner_no_thomas_baker ; rand_score =  0.02247191011235955\n",
      "Model  primitive ; rand_score =  0.001011122345803871\n",
      "Model  trigrams ; rand_score =  0.001011122345803871\n",
      "Model  bigrams ; rand_score =  0.001011122345803871\n",
      "Model  lemmatized ; rand_score =  0.001011122345803871\n",
      "Model  no_stop_words ; rand_score =  0.001011122345803871\n",
      "Model  no_tomas_baker ; rand_score =  0.001011122345803871\n",
      "Model  no_repeated_words ; rand_score =  0.001011122345803871\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  -0.002244668911335642\n",
      "Model  identity_analysis_1 ; rand_score =  -0.03636363636363637\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  -0.044847837693539755\n",
      "Model  no_tomas_stemmed ; rand_score =  -0.044847837693539755\n",
      "Model  stemmed_txts ; rand_score =  -0.044847837693539755\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  -0.044847837693539755\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "**************************************\n",
      "Getting results for the clustering mode  AgglomerativeClustering\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.6341463414634146\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.5140073081607796\n",
      "Model  stanford_ner_no_thomas_baker ; rand_score =  0.38159371492704824\n",
      "Model  trigrams ; rand_score =  0.27565982404692085\n",
      "Model  no_repeated_words ; rand_score =  0.23918846769887883\n",
      "Model  stanford_ner_txts ; rand_score =  0.1354903943377149\n",
      "Model  bigrams ; rand_score =  0.09759271307742352\n",
      "Model  identity_analysis_2 ; rand_score =  0.05659369994660974\n",
      "Model  identity_analysis_1 ; rand_score =  0.001011122345803871\n",
      "Model  lemmatized ; rand_score =  0.001011122345803871\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  -0.002244668911335642\n",
      "Model  no_tomas_stemmed ; rand_score =  -0.002244668911335642\n",
      "Model  stemmed_txts ; rand_score =  -0.002244668911335642\n",
      "Model  no_stop_words ; rand_score =  -0.06576402321083177\n",
      "Model  no_tomas_baker ; rand_score =  -0.0809384164222874\n",
      "Model  primitive ; rand_score =  -0.11351017890191237\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "CLASIFICATION WITH 4 CLUSTERS\n",
      "**************************************\n",
      "Getting results for the clustering mode  KMeans\n",
      "Model  identity_analysis_2 ; rand_score =  0.2304469273743017\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  0.17503392130257805\n",
      "Model  primitive ; rand_score =  0.17503392130257805\n",
      "Model  no_stop_words ; rand_score =  0.1492537313432836\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.14703968770331813\n",
      "Model  no_tomas_baker ; rand_score =  0.12347354138398917\n",
      "Model  lemmatized ; rand_score =  0.10494931425163982\n",
      "Model  no_repeated_words ; rand_score =  0.0836012861736335\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.07255936675461744\n",
      "Model  stanford_ner_no_thomas_baker ; rand_score =  0.05510752688172045\n",
      "Model  stanford_ner_txts ; rand_score =  0.051395007342143924\n",
      "Model  no_tomas_stemmed ; rand_score =  0.020352781546811426\n",
      "Model  stemmed_txts ; rand_score =  0.020352781546811426\n",
      "Model  identity_analysis_1 ; rand_score =  -0.05698778833107188\n",
      "Model  trigrams ; rand_score =  -0.058949624866023516\n",
      "Model  bigrams ; rand_score =  -0.058949624866023516\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "**************************************\n",
      "Getting results for the clustering mode  MiniBatchKMeans\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.2835195530726257\n",
      "Model  identity_analysis_2 ; rand_score =  0.2835195530726257\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  0.17503392130257805\n",
      "Model  trigrams ; rand_score =  0.16176470588235295\n",
      "Model  no_stop_words ; rand_score =  0.1492537313432836\n",
      "Model  no_tomas_stemmed ; rand_score =  0.12347354138398917\n",
      "Model  stemmed_txts ; rand_score =  0.12347354138398917\n",
      "Model  primitive ; rand_score =  0.12347354138398917\n",
      "Model  no_tomas_baker ; rand_score =  0.12347354138398917\n",
      "Model  stanford_ner_no_thomas_baker ; rand_score =  0.07374999999999998\n",
      "Model  stanford_ner_txts ; rand_score =  0.051395007342143924\n",
      "Model  lemmatized ; rand_score =  -0.058949624866023516\n",
      "Model  identity_analysis_1 ; rand_score =  -0.060570071258907406\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  -0.060570071258907406\n",
      "Model  bigrams ; rand_score =  -0.08302354399008675\n",
      "Model  no_repeated_words ; rand_score =  -0.09615384615384613\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "**************************************\n",
      "Getting results for the clustering mode  AgglomerativeClustering\n",
      "Model  no_tomas_stemmed_ent1 ; rand_score =  0.7446236559139784\n",
      "Model  named_ent_2_no_tomas_barker ; rand_score =  0.7446236559139784\n",
      "Model  trigrams ; rand_score =  0.49316851008458035\n",
      "Model  stanford_ner_no_thomas_baker ; rand_score =  0.4809976247030879\n",
      "Model  stanford_ner_txts ; rand_score =  0.34076827757125155\n",
      "Model  no_repeated_words ; rand_score =  0.33371040723981904\n",
      "Model  bigrams ; rand_score =  0.20233998623537508\n",
      "Model  identity_analysis_1 ; rand_score =  0.16176470588235295\n",
      "Model  lemmatized ; rand_score =  0.06323687031082535\n",
      "Model  identity_analysis_2 ; rand_score =  0.06323687031082535\n",
      "Model  no_tomas_stemmed ; rand_score =  -0.060570071258907406\n",
      "Model  no_tomas_stemmed_no_stop ; rand_score =  -0.09250000000000001\n",
      "Model  stemmed_txts ; rand_score =  -0.09250000000000001\n",
      "Model  no_stop_words ; rand_score =  -0.10854816824966075\n",
      "Model  no_tomas_baker ; rand_score =  -0.1301115241635688\n",
      "Model  primitive ; rand_score =  -0.1496421600520495\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "/////////////////////////////////\n",
      "CLUSTER ALGORITHM:  AgglomerativeClustering\n",
      "*Model: \" no_tomas_stemmed_no_stop \". Best cluster agroupation:  2  clusters. Score:  0.06676204101096801\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_tomas_stemmed_no_stop  is  -0.09250000000000001\n",
      "*Model: \" identity_analysis_1 \". Best cluster agroupation:  4  clusters. Score:  0.16176470588235295\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  identity_analysis_1  is  0.16176470588235295\n",
      "*Model: \" no_tomas_stemmed \". Best cluster agroupation:  2  clusters. Score:  0.06676204101096801\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_tomas_stemmed  is  -0.060570071258907406\n",
      "*Model: \" stemmed_txts \". Best cluster agroupation:  2  clusters. Score:  0.06676204101096801\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  stemmed_txts  is  -0.09250000000000001\n",
      "*Model: \" primitive \". Best cluster agroupation:  2  clusters. Score:  -0.022185246810870803\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  primitive  is  -0.1496421600520495\n",
      "*Model: \" trigrams \". Best cluster agroupation:  4  clusters. Score:  0.49316851008458035\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  trigrams  is  0.49316851008458035\n",
      "*Model: \" bigrams \". Best cluster agroupation:  4  clusters. Score:  0.20233998623537508\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  bigrams  is  0.20233998623537508\n",
      "*Model: \" no_tomas_stemmed_ent1 \". Best cluster agroupation:  4  clusters. Score:  0.7446236559139784\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_tomas_stemmed_ent1  is  0.7446236559139784\n",
      "*Model: \" named_ent_2_no_tomas_barker \". Best cluster agroupation:  4  clusters. Score:  0.7446236559139784\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  named_ent_2_no_tomas_barker  is  0.7446236559139784\n",
      "*Model: \" lemmatized \". Best cluster agroupation:  2  clusters. Score:  0.06676204101096801\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  lemmatized  is  0.06323687031082535\n",
      "*Model: \" no_stop_words \". Best cluster agroupation:  2  clusters. Score:  -0.022185246810870803\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_stop_words  is  -0.10854816824966075\n",
      "*Model: \" no_tomas_baker \". Best cluster agroupation:  2  clusters. Score:  -0.022185246810870803\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_tomas_baker  is  -0.1301115241635688\n",
      "*Model: \" stanford_ner_no_thomas_baker \". Best cluster agroupation:  4  clusters. Score:  0.4809976247030879\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  stanford_ner_no_thomas_baker  is  0.4809976247030879\n",
      "*Model: \" identity_analysis_2 \". Best cluster agroupation:  2  clusters. Score:  0.06676204101096801\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  identity_analysis_2  is  0.06323687031082535\n",
      "*Model: \" no_repeated_words \". Best cluster agroupation:  4  clusters. Score:  0.33371040723981904\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_repeated_words  is  0.33371040723981904\n",
      "*Model: \" stanford_ner_txts \". Best cluster agroupation:  4  clusters. Score:  0.34076827757125155\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  stanford_ner_txts  is  0.34076827757125155\n",
      "/////////////////////////////////\n",
      "\n",
      "/////////////////////////////////\n",
      "CLUSTER ALGORITHM:  KMeans\n",
      "*Model: \" no_tomas_stemmed_no_stop \". Best cluster agroupation:  4  clusters. Score:  0.17503392130257805\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_tomas_stemmed_no_stop  is  0.17503392130257805\n",
      "*Model: \" identity_analysis_1 \". Best cluster agroupation:  2  clusters. Score:  0.06676204101096801\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  identity_analysis_1  is  -0.05698778833107188\n",
      "*Model: \" no_tomas_stemmed \". Best cluster agroupation:  2  clusters. Score:  0.06676204101096801\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_tomas_stemmed  is  0.020352781546811426\n",
      "*Model: \" stemmed_txts \". Best cluster agroupation:  3  clusters. Score:  0.14703968770331813\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  stemmed_txts  is  0.020352781546811426\n",
      "*Model: \" primitive \". Best cluster agroupation:  4  clusters. Score:  0.17503392130257805\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  primitive  is  0.17503392130257805\n",
      "*Model: \" trigrams \". Best cluster agroupation:  2  clusters. Score:  0.06676204101096801\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  trigrams  is  -0.058949624866023516\n",
      "*Model: \" bigrams \". Best cluster agroupation:  2  clusters. Score:  0.06676204101096801\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  bigrams  is  -0.058949624866023516\n",
      "*Model: \" no_tomas_stemmed_ent1 \". Best cluster agroupation:  4  clusters. Score:  0.14703968770331813\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_tomas_stemmed_ent1  is  0.14703968770331813\n",
      "*Model: \" named_ent_2_no_tomas_barker \". Best cluster agroupation:  3  clusters. Score:  0.5032679738562091\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  named_ent_2_no_tomas_barker  is  0.07255936675461744\n",
      "*Model: \" lemmatized \". Best cluster agroupation:  3  clusters. Score:  0.23918846769887883\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  lemmatized  is  0.10494931425163982\n",
      "*Model: \" no_stop_words \". Best cluster agroupation:  4  clusters. Score:  0.1492537313432836\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_stop_words  is  0.1492537313432836\n",
      "*Model: \" no_tomas_baker \". Best cluster agroupation:  4  clusters. Score:  0.12347354138398917\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_tomas_baker  is  0.12347354138398917\n",
      "*Model: \" stanford_ner_no_thomas_baker \". Best cluster agroupation:  4  clusters. Score:  0.05510752688172045\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  stanford_ner_no_thomas_baker  is  0.05510752688172045\n",
      "*Model: \" identity_analysis_2 \". Best cluster agroupation:  4  clusters. Score:  0.2304469273743017\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  identity_analysis_2  is  0.2304469273743017\n",
      "*Model: \" no_repeated_words \". Best cluster agroupation:  4  clusters. Score:  0.0836012861736335\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_repeated_words  is  0.0836012861736335\n",
      "*Model: \" stanford_ner_txts \". Best cluster agroupation:  3  clusters. Score:  0.05673758865248223\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  stanford_ner_txts  is  0.051395007342143924\n",
      "/////////////////////////////////\n",
      "\n",
      "/////////////////////////////////\n",
      "CLUSTER ALGORITHM:  MiniBatchKMeans\n",
      "*Model: \" no_tomas_stemmed_no_stop \". Best cluster agroupation:  4  clusters. Score:  0.17503392130257805\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_tomas_stemmed_no_stop  is  0.17503392130257805\n",
      "*Model: \" identity_analysis_1 \". Best cluster agroupation:  2  clusters. Score:  0.06676204101096801\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  identity_analysis_1  is  -0.060570071258907406\n",
      "*Model: \" no_tomas_stemmed \". Best cluster agroupation:  4  clusters. Score:  0.12347354138398917\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_tomas_stemmed  is  0.12347354138398917\n",
      "*Model: \" stemmed_txts \". Best cluster agroupation:  4  clusters. Score:  0.12347354138398917\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  stemmed_txts  is  0.12347354138398917\n",
      "*Model: \" primitive \". Best cluster agroupation:  4  clusters. Score:  0.12347354138398917\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  primitive  is  0.12347354138398917\n",
      "*Model: \" trigrams \". Best cluster agroupation:  4  clusters. Score:  0.16176470588235295\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  trigrams  is  0.16176470588235295\n",
      "*Model: \" bigrams \". Best cluster agroupation:  2  clusters. Score:  0.06676204101096801\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  bigrams  is  -0.08302354399008675\n",
      "*Model: \" no_tomas_stemmed_ent1 \". Best cluster agroupation:  2  clusters. Score:  0.06676204101096801\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_tomas_stemmed_ent1  is  -0.060570071258907406\n",
      "*Model: \" named_ent_2_no_tomas_barker \". Best cluster agroupation:  4  clusters. Score:  0.2835195530726257\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  named_ent_2_no_tomas_barker  is  0.2835195530726257\n",
      "*Model: \" lemmatized \". Best cluster agroupation:  2  clusters. Score:  0.06676204101096801\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  lemmatized  is  -0.058949624866023516\n",
      "*Model: \" no_stop_words \". Best cluster agroupation:  4  clusters. Score:  0.1492537313432836\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_stop_words  is  0.1492537313432836\n",
      "*Model: \" no_tomas_baker \". Best cluster agroupation:  4  clusters. Score:  0.12347354138398917\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_tomas_baker  is  0.12347354138398917\n",
      "*Model: \" stanford_ner_no_thomas_baker \". Best cluster agroupation:  4  clusters. Score:  0.07374999999999998\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  stanford_ner_no_thomas_baker  is  0.07374999999999998\n",
      "*Model: \" identity_analysis_2 \". Best cluster agroupation:  4  clusters. Score:  0.2835195530726257\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  identity_analysis_2  is  0.2835195530726257\n",
      "*Model: \" no_repeated_words \". Best cluster agroupation:  2  clusters. Score:  0.06676204101096801\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  no_repeated_words  is  -0.09615384615384613\n",
      "*Model: \" stanford_ner_txts \". Best cluster agroupation:  3  clusters. Score:  0.15814587593728696\n",
      "++Score  for the real cluster agroupation ( 4 ) in model  stanford_ner_txts  is  0.051395007342143924\n",
      "/////////////////////////////////\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tested_models = {}\n",
    "top_cluster = 4\n",
    "best_scores_all_clusters = {}\n",
    "best_scores_Realcluster = {}\n",
    "real_cluster_grouping = 4\n",
    "init_scores = -999999\n",
    "for cluster_mode in clustering_modes:\n",
    "    best_scores_all_clusters[cluster_mode] = {}\n",
    "    best_scores_Realcluster[cluster_mode] = {}\n",
    "\n",
    "for cluster in range(1,top_cluster+1):\n",
    "    fix_grouping = cluster\n",
    "    print('CLASIFICATION WITH ' + str(fix_grouping) + ' CLUSTERS')\n",
    "    for cluster_mode in clustering_modes:\n",
    "        tested_models[cluster_mode] = {}        \n",
    "        tested_models[cluster_mode][\"primitive\"] = cluster_texts(raw_texts,fix_grouping,distanceFunction, cluster_mode)\n",
    "        tested_models[cluster_mode][\"identity_analysis_1\"] = cluster_texts(named_ent_txts_1,fix_grouping,distanceFunction, cluster_mode)\n",
    "        tested_models[cluster_mode][\"stemmed_txts\"] = cluster_texts(stemmed_txts,fix_grouping,distanceFunction, cluster_mode)\n",
    "        tested_models[cluster_mode][\"no_tomas_baker\"] = cluster_texts(texts_no_thomas_baker,fix_grouping,distanceFunction, cluster_mode)\n",
    "        tested_models[cluster_mode][\"no_stop_words\"] = cluster_texts(texts_no_stop_words,fix_grouping,distanceFunction, cluster_mode)\n",
    "        tested_models[cluster_mode][\"no_tomas_stemmed\"] = cluster_texts(no_tomas_stemmed_txts,fix_grouping,distanceFunction, cluster_mode)\n",
    "        tested_models[cluster_mode][\"no_tomas_stemmed_no_stop\"] = cluster_texts(no_tomas_stemmed_no_stop_txts,fix_grouping,\n",
    "                                                                                distanceFunction, cluster_mode)\n",
    "        tested_models[cluster_mode][\"no_tomas_stemmed_ent1\"] = cluster_texts(no_tomas_stemmed_ent1_txts,fix_grouping,distanceFunction, \n",
    "                                                                             cluster_mode)\n",
    "        tested_models[cluster_mode][\"identity_analysis_2\"] = cluster_texts(named_ent_txts_2,fix_grouping,distanceFunction, cluster_mode)\n",
    "        tested_models[cluster_mode][\"named_ent_2_no_tomas_barker\"] = cluster_texts(named_ent_2_no_tomas_barker_txts,\n",
    "                                                                 fix_grouping,distanceFunction, cluster_mode)\n",
    "        tested_models[cluster_mode]['bigrams'] = cluster_texts(bigrams_texts,fix_grouping,distanceFunction, cluster_mode)\n",
    "        tested_models[cluster_mode]['trigrams'] = cluster_texts(trigrams_texts, fix_grouping,distanceFunction, cluster_mode)\n",
    "        tested_models[cluster_mode]['lemmatized'] = cluster_texts(lemmatized_texts, fix_grouping,distanceFunction, cluster_mode)\n",
    "        tested_models[cluster_mode]['no_repeated_words'] = cluster_texts(no_repeatedWords_noStopWords,\n",
    "                                                                         fix_grouping, distanceFunction, cluster_mode)\n",
    "        \n",
    "        \n",
    "        tested_models[cluster_mode]['stanford_ner_txts'] = cluster_texts(stanford_ner_txts,fix_grouping, \n",
    "                                                                     distanceFunction, cluster_mode)\n",
    "        tested_models[cluster_mode]['stanford_ner_no_thomas_baker'] = cluster_texts(stanford_ner_no_thomas_baker,fix_grouping, \n",
    "                                                                     distanceFunction, cluster_mode)\n",
    "                \n",
    "\n",
    "    # Evaluation\n",
    "    tested_models_scores = {}\n",
    "\n",
    "    for cluster_mode in tested_models:\n",
    "        tested_models_scores[cluster_mode] = {}        \n",
    "        for model in tested_models[cluster_mode]:            \n",
    "            tested_models_scores[cluster_mode][model] = adjusted_rand_score(reference,tested_models[cluster_mode][model])\n",
    "            \n",
    "            # Calculate the max score for each model, each custer amount\n",
    "            if model in best_scores_all_clusters[cluster_mode].keys():\n",
    "                if best_scores_all_clusters[cluster_mode][model]['score'] <= tested_models_scores[cluster_mode][model]:\n",
    "                    best_scores_all_clusters[cluster_mode][model]['cluster'] = cluster\n",
    "                    best_scores_all_clusters[cluster_mode][model]['score'] = tested_models_scores[cluster_mode][model] \n",
    "            else:\n",
    "                best_scores_all_clusters[cluster_mode][model] = {}\n",
    "                best_scores_all_clusters[cluster_mode][model]['cluster'] = cluster\n",
    "                best_scores_all_clusters[cluster_mode][model]['score'] = init_scores    \n",
    "            \n",
    "            if cluster == real_cluster_grouping:\n",
    "                best_scores_Realcluster[cluster_mode][model] = {}\n",
    "                best_scores_Realcluster[cluster_mode][model]['cluster'] = cluster\n",
    "                best_scores_Realcluster[cluster_mode][model]['score'] = tested_models_scores[cluster_mode][model]              \n",
    "    \n",
    "    for cluster_mode in tested_models_scores:\n",
    "        print(\"**************************************\")\n",
    "        print(\"Getting results for the clustering mode \", cluster_mode)\n",
    "        for model in sorted(tested_models_scores[cluster_mode].items(), key=operator.itemgetter(1), reverse=True):\n",
    "            print(\"Model \", model[0], \"; rand_score = \", model[1])            \n",
    "        print(\"########################################\")\n",
    "        print(\"########################################\\n\")\n",
    "\n",
    "for cluster_mode in clustering_modes:\n",
    "    print('/////////////////////////////////')\n",
    "    print('CLUSTER ALGORITHM: ', cluster_mode)\n",
    "    for model in best_scores_all_clusters[cluster_mode].keys():\n",
    "                        \n",
    "        print('*Model: \"', model, '\". Best cluster agroupation: ',\n",
    "                  best_scores_all_clusters[cluster_mode][model]['cluster'],\n",
    "                  ' clusters. Score: ', \n",
    "                  str(best_scores_all_clusters[cluster_mode][model]['score'])) \n",
    "        \n",
    "        print('++Score  for the real cluster agroupation (',str(real_cluster_grouping),\n",
    "              ') in model ', model, ' is ',\n",
    "              str(best_scores_Realcluster[cluster_mode][model]['score'])) \n",
    "    print('/////////////////////////////////\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# En la memoria se ha de justificar cada cambio propuesto.\n",
    "\n",
    "# Se ha de comentar los resultados obtenidos.\n",
    "\n",
    "# Siempre buscando una mejora en la clusterización"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
