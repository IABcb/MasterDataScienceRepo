{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Práctica 1. Diferenciación de entidades.\n",
    "## Autores: \n",
    "     Raúl Sánchez Martín\n",
    "     Ignacio Arias Barra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re, pprint, os, numpy\n",
    "# -*- coding: utf-8 -*-\n",
    "import nltk\n",
    "import string\n",
    "path_to_append = '/media/nacho/f8371289-0f00-4406-89db-d575f3cdb35e/Master/Trimestre 2/RIM/nltk_data'\n",
    "nltk.data.path.append(path_to_append)\n",
    "from sklearn.metrics.cluster import *\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from nltk.cluster import GAAClusterer\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Código origen\n",
    "Lectura de fichero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    myfile = open(file,\"r\")\n",
    "    data = \"\"\n",
    "    lines = myfile.readlines()\n",
    "    for line in lines:\n",
    "        data = data + line\n",
    "    myfile.close\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cluster_texts(texts, clustersNumber, distance):\n",
    "    # Convierte textos en una coleccion\n",
    "    # Load the list of texts into a TextCollection object.\n",
    "    collection = nltk.TextCollection(texts)    \n",
    "#     print(\"Created a collection of\", len(collection), \"terms.\")\n",
    "    \n",
    "    # Get a list of unique terms\n",
    "    unique_terms = list(set(collection))\n",
    "#     print(\"Unique terms found: \", len(unique_terms))\n",
    "\n",
    "    ### And here we actually call the function and create our array of vectors.\n",
    "    # TF mide la frecuencia en los textos.\n",
    "    # Mira de los terminos unicos, cuantas veces aparece en el documento. No mira cuantas veces aparece en la coleccion\n",
    "    # Hay otras medidas, como TF-IDF que son mas precisas porque tambien miran cuantas veces aparece en la coleccion\n",
    "    vectors = [numpy.array(TF(f,unique_terms, collection)) for f in texts]\n",
    "    # print(\"Vectors created.\")\n",
    "    # print(vectors)\n",
    "\n",
    "    # initialize the clusterer\n",
    "    clusterer = GAAClusterer(clustersNumber)\n",
    "    clusters = clusterer.cluster(vectors, True)\n",
    "    # Estas lineas siguientes comentadas es lo mismo pero con otra libreria, la llamada scikit-learn\n",
    "    # clusterer = AgglomerativeClustering(n_clusters=clustersNumber,\n",
    "    #                                  linkage=\"average\", affinity=distanceFunction)\n",
    "    #clusters = clusterer.fit_predict(vectors)\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Frecuencia de términos únicos en un documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Function to create a TF vector for one document. For each of\n",
    "# our unique words, we have a feature which is the tf for that word\n",
    "# in the current document\n",
    "def TF(document, unique_terms, collection):\n",
    "    word_tf = []\n",
    "    for word in unique_terms:\n",
    "        word_tf.append(collection.tf(word, document))\n",
    "    return word_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Nuevo código\n",
    "\n",
    "*EVALUACIÓN DIFERENTES TAMAÑOS DE CLUSTER*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def eval_test(top_test, priori_groups, reference):\n",
    "    score = 0\n",
    "    curr_score = -9999999\n",
    "    curr_priori_groups = 1\n",
    "    for n in range(top_test):\n",
    "        test = cluster_texts(texts,priori_groups,distanceFunction)\n",
    "        # print(\"test: \", test)\n",
    "        # Evaluation\n",
    "        score = adjusted_rand_score(reference, test)\n",
    "\n",
    "        print(\"group: \", priori_groups)\n",
    "        print(\"rand_score: \", score)\n",
    "\n",
    "        if score >= curr_score:\n",
    "            curr_score = score\n",
    "            curr_priori_groups = priori_groups\n",
    "\n",
    "        priori_groups+=1\n",
    "\n",
    "    # Gold Standard\n",
    "    # el documento que meto el primero sera igual que los que esten en todos los ceros\n",
    "    print(\"\\nBest agroupation: \", curr_priori_groups)\n",
    "    print(\"Best score: \", curr_score)\n",
    "    print(\"reference: \", reference)\n",
    "    return curr_priori_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*USO DE ENTIDADES*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*ELIMINAR PALABRAS VACÍAS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# # Abrimos el archivo.\n",
    "# with open('./Thomas_Baker/005.txt', 'r', encoding=\"utf8\") as f:\n",
    "# # with open('./Thomas_Baker/036.txt', 'r', encoding=\"utf8\") as f:\n",
    "#     text = f.read()\n",
    "# f.close()\n",
    "\n",
    "\n",
    "def delete_stopwords(text,lan='en'):\n",
    "\n",
    "    if lan == 'en':        \n",
    "        # Obtenemos las sentencias del texto.\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        # Obtenemos las \"stopwords\" del inglés.\n",
    "        stop = set(stopwords.words('english'))\n",
    "\n",
    "    elif lan == 'es':        \n",
    "        # Elegimos el tokenizador para el español de la NLTK.\n",
    "        spanish_tokenizer = nltk.data.load('tokenizers/punkt/spanish.pickle')\n",
    "        # Obtenemos las sentencias del texto.\n",
    "        sentences = spanish_tokenizer.tokenize(text)        \n",
    "\n",
    "        # Obtenemos las \"stopwords\" del español.\n",
    "        stop = set(stopwords.words('spanish'))\n",
    "      \n",
    "    print(\"Frases originales: \")\n",
    "    print(sentences)\n",
    "    \n",
    "    stopw = []\n",
    "    for i in stop:\n",
    "        stopw.append(i)\n",
    "    print(\"Las stop words en lan = \" + lan)\n",
    "    print(stopw)\n",
    "    print()\n",
    "\n",
    "    # Eliminamos las palabras que coinciden con alguna \"stopword\".\n",
    "    print(\"Frases procesadas sin stopwords: \")\n",
    "    for sentence in sentences:\n",
    "        non_stop_sentence = \"\"\n",
    "        for word in sentence.lower().split():\n",
    "            if word not in stop:\n",
    "                non_stop_sentence = non_stop_sentence + word + \" \"\n",
    "        # Imprimimos las sentencias procesadas.\n",
    "        print(non_stop_sentence)\n",
    "    return non_stop_sentence\n",
    "\n",
    "# espanol = delete_stopwords(text,lan='es')\n",
    "# ingles = delete_stopwords(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*ELIMINAR EL NOMBRE DE LA PERSONA EN LOS TEXTOS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*REPRESENTACIÓN DE NGRAMAS A NIVEL DE PALABRAS*\n",
    "\n",
    "(trigramas es poner casa-perro-coche en las ocurrencias por texto),\n",
    " se puede hacer los ngramas con nltk y scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*STEAMMING*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# # Abrimos el archivo.\n",
    "# with open('./Thomas_Baker/005.txt', 'r', encoding=\"utf8\") as f:\n",
    "# with open('./Thomas_Baker/036.txt', 'r', encoding=\"utf8\") as f:\n",
    "#     text = f.read()\n",
    "# f.close()\n",
    "\n",
    "def stemming(text, lan='en'):\n",
    "    stemmeds = []\n",
    "    if lan == 'en':\n",
    "        # Steamer ingles\n",
    "        stemmer = PorterStemmer()        \n",
    "    elif lan == 'es':\n",
    "        # Stemer espanol        \n",
    "        stemmer = SnowballStemmer(\"spanish\")\n",
    "    \n",
    "    # Obtenemos los tokens del texto.\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "        \n",
    "    # Para cada token del texto obtenemos su raíz.\n",
    "    for token in tokens:\n",
    "        stemmed = stemmer.stem(token)\n",
    "        stemmeds.append(stemmed)\n",
    "    # Escribimos el resultado para compararlo con las palabras originales.\n",
    "    print(\"Palabras originales: \")\n",
    "    print(tokens)\n",
    "    print(\"Palabras procesadas con Snowball: \")\n",
    "    print(stemmeds)\n",
    "    \n",
    "# stemming(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*EXPLORAR CONTENIDO DE LAS PÁGINAS WEB*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Inicio programa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared  19  documents...\n",
      "They can be accessed using texts[0] - texts[18]\n",
      "group:  1\n",
      "rand_score:  0.0\n",
      "group:  2\n",
      "rand_score:  -0.106489184692\n",
      "group:  3\n",
      "rand_score:  -0.0900678593461\n",
      "group:  4\n",
      "rand_score:  -0.100195185426\n",
      "group:  5\n",
      "rand_score:  -0.0928104575163\n",
      "group:  6\n",
      "rand_score:  0.010719754977\n",
      "group:  7\n",
      "rand_score:  0.0477326968974\n",
      "group:  8\n",
      "rand_score:  -0.0145631067961\n",
      "group:  9\n",
      "rand_score:  -0.0164609053498\n",
      "group:  10\n",
      "rand_score:  -0.059950041632\n",
      "\n",
      "Best agroupation:  7\n",
      "Best score:  0.0477326968974\n",
      "reference:  [0, 1, 2, 0, 0, 0, 3, 0, 0, 0, 2, 0, 3, 3, 0, 1, 2, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Folder with all texts\n",
    "folder = \"Thomas_Baker\"\n",
    "# Empty list to hold text documents.\n",
    "texts = []\n",
    "\n",
    "listing = os.listdir(folder)\n",
    "for file in listing:\n",
    "    if file.endswith(\".txt\"):\n",
    "        url = folder+\"/\"+file\n",
    "        f = open(url,encoding=\"latin-1\");\n",
    "        raw = f.read()\n",
    "        f.close()\n",
    "        tokens = nltk.word_tokenize(raw)\n",
    "        text = nltk.Text(tokens)\n",
    "        texts.append(text)\n",
    "\n",
    "print(\"Prepared \", len(texts), \" documents...\")\n",
    "print(\"They can be accessed using texts[0] - texts[\" + str(len(texts)-1) + \"]\")\n",
    "\n",
    "# Similarity distance\n",
    "distanceFunction =\"cosine\"\n",
    "# distanceFunction = \"euclidean\"\n",
    "\n",
    "# iterations for the evaluation of clusters\n",
    "top_test = 10\n",
    "priori_groups = 1\n",
    "\n",
    "# Final solution for comparing in tests\n",
    "reference = [0, 1, 2, 0, 0, 0, 3, 0, 0, 0, 2, 0, 3, 3, 0, 1, 2, 0, 1]\n",
    "groups = eval_test(top_test, priori_groups, reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# En la memoria se ha de justificar cada cambio propuesto.\n",
    "\n",
    "# Se ha de comentar los resultados obtenidos.\n",
    "\n",
    "# Siempre buscando una mejora en la clusterización"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
