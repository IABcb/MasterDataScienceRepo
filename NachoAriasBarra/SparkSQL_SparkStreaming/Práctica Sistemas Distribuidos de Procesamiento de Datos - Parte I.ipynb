{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica Spark, Apache Kafka, Spark SQL\n",
    "\n",
    "Autor: Ignacio Arias Barra\n",
    "\n",
    "En la presente práctica se va a llevar a cabo el análisis de un dataset mediante técnicas de procesamiento de datos específicas de la tecnología Spark. Concretamente, Spark SQL y Spark Streaming. La base de datos a utilizar, se llama DATASETMotoGPQatar.csv, la cual contiene información sobre tweets que se escribieron durante el gran premio de moto GP.\n",
    "\n",
    "### Spark context\n",
    "\n",
    "En primer lugar definimos el contexto de spark y spark streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f35172b6198>\n",
      "<pyspark.context.SparkContext object at 0x7f3546cb64a8>\n"
     ]
    }
   ],
   "source": [
    "# Spark\n",
    "import findspark\n",
    "spark_path = \"/home/nacho/spark\"\n",
    "findspark.init(spark_path)\n",
    "import re\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import shutil\n",
    "import os\n",
    "import signal\n",
    "import sys\n",
    "import subprocess\n",
    "spark = (SparkSession.builder\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.cores\", 1)\n",
    "    .appName(\"understanding_sparksession\")\n",
    "    .getOrCreate() )\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(spark)\n",
    "print(sc)\n",
    "\n",
    "# Spark Streaming\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from operator import add\n",
    "from operator import sub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición del esquema\n",
    "\n",
    "Para la correcta lectura de la base de datos, necesitamos definir el esquema de la misma, aportando la tipología de las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "customSchema = StructType([StructField(\"Id\", LongType(), True),\n",
    "                           StructField(\"Parent_sys_id\", StringType(), True),\n",
    "                           StructField(\"Source\", StringType(), True),\n",
    "                           StructField(\"Mentions\", StringType(), True),\n",
    "                           StructField(\"Target\", StringType(), True),\n",
    "                           StructField(\"Name_source\", StringType(), True),\n",
    "                           StructField(\"Body\", StringType(), True),\n",
    "                           StructField(\"Pub_date\", TimestampType(), True),\n",
    "                           StructField(\"URLs\", StringType(), True),\n",
    "                           StructField(\"Tipe_action\", StringType(), True),\n",
    "                           StructField(\"Link\", StringType(), True),\n",
    "                           StructField(\"Has_link\", ByteType(), True),\n",
    "                           StructField(\"Has_picture\", ByteType(), True),\n",
    "                           StructField(\"Website\", StringType(), True),\n",
    "                           StructField(\"Country\", StringType(), True),\n",
    "                           StructField(\"Activity\", LongType(), True),\n",
    "                           StructField(\"Followers\", LongType(), True),\n",
    "                           StructField(\"Following\", LongType(), True),\n",
    "                           StructField(\"Location\", StringType(), True)\n",
    "                          ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lectura de eventos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sep = '\\t'\n",
    "data_path = \"data/DATASETMotoGP-Qatar.csv\"\n",
    "events = spark.read.csv(data_path, header=True, schema=customSchema, timestampFormat=\"dd/MM/yyyy HH:mm\", sep = sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualización del esquema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: long (nullable = true)\n",
      " |-- Parent_sys_id: string (nullable = true)\n",
      " |-- Source: string (nullable = true)\n",
      " |-- Mentions: string (nullable = true)\n",
      " |-- Target: string (nullable = true)\n",
      " |-- Name_source: string (nullable = true)\n",
      " |-- Body: string (nullable = true)\n",
      " |-- Pub_date: timestamp (nullable = true)\n",
      " |-- URLs: string (nullable = true)\n",
      " |-- Tipe_action: string (nullable = true)\n",
      " |-- Link: string (nullable = true)\n",
      " |-- Has_link: byte (nullable = true)\n",
      " |-- Has_picture: byte (nullable = true)\n",
      " |-- Website: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Activity: long (nullable = true)\n",
      " |-- Followers: long (nullable = true)\n",
      " |-- Following: long (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "events.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cuestiones\n",
    "\n",
    "##### A) Contabilizar el número total de menciones a los pilotos Marc Márquez, Valentino Rossi y Dani Pedrosa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58117"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Marc_id = 'marcmarquez93'\n",
    "\n",
    "(events.select('Mentions')\n",
    ".filter(events.Mentions.rlike(Marc_id))\n",
    ".count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61121"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Valen_id = 'valeyellow46'\n",
    "\n",
    "(events.select('Mentions')\n",
    ".filter(events.Mentions.rlike(Valen_id))\n",
    ".count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12342"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dani_id = '26_danipedrosa'\n",
    "\n",
    "(events.select('Mentions')\n",
    ".filter(events.Mentions.rlike(Dani_id))\n",
    ".count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### B) Contabilizar los 5 países que más tweets han publicado (considerando los tweets que contengan dicha información)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|Country|tweets|\n",
      "+-------+------+\n",
      "|     es|172577|\n",
      "|     us| 12722|\n",
      "|     gb| 12588|\n",
      "|     id|  8725|\n",
      "|     it|  1843|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries_toshow = 5\n",
    "(events.filter(events.Country != \"not public\")\n",
    " .groupBy(\"Country\")\n",
    " .agg(count(\"Id\").alias(\"tweets\"))\n",
    " .orderBy(\"tweets\", ascending=False)\n",
    " .limit(countries_toshow).show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C) Contabilizar los 3 hashtags más utilizados (que aparezcan el mayor número de veces) en el cuerpo de los tweets (campo \"body\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "| Hastag|Repetitions|\n",
      "+-------+-----------+\n",
      "|#motogp|      51961|\n",
      "| #qatar|       9977|\n",
      "| #moto3|       5797|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hastags_toshow = 3\n",
    "hastag_sym = '#'\n",
    "\n",
    "(events.select('Body')\n",
    " .rdd\n",
    " .flatMap(lambda body: body.Body.split(' ') if body.Body is not None else 'empty')\n",
    " .map(lambda word: (word, 1) if word.startswith(hastag_sym) else (word, 0))\n",
    " .reduceByKey(lambda c1, c2: c1 + c2)\n",
    " .toDF()\n",
    " .select(col('_1').alias('Hastag'), col('_2').alias('Repetitions'))\n",
    " .orderBy('Repetitions', ascending=False)\n",
    " .limit(hastags_toshow)\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Streaming + Apache Kafka\n",
    "\n",
    "Para la ejecución de procesado streaming, ademáś de Spark, necesitamos de otras tecnologías como es el caso de Apache Kafka y Zookeeper.\n",
    "\n",
    "Con la finalidad los métodos de instalación y uso de estas tecnologías, se ha optado por ejecutar un docker, en cuyo interior se ejecutarán tanto el broker kafka como zookeeper.\n",
    "\n",
    "Este docker ha sido descargado de su repositorio en gitHub: *https://github.com/spotify/docker-kafka.git*\n",
    "\n",
    "Una vez descargado, debemos ejecutar el siguiente comando en la terminal:\n",
    "\n",
    "**sudo docker run -itd --name=kafka -p 2181:2181 -p 9092:9092 --env ADVERTISED_HOST=localhost --env ADVERTISED_PORT=9092 --env CONSUMER_THREADS=10 spotify/kafka**\n",
    "\n",
    "Para llevar a cabo estos pasos de ejecución del docker, se presupone instalado el docker engine correspondiente.\n",
    "\n",
    "El siguiente paso a tener en cuenta, es la activación de un producer. Éste se encargará de leer las líneas del fichero de la base de datos que contiene la información sobre los tweets y la enviará a kafka mediante el topic *Quatar_GP_2014*.\n",
    "\n",
    "La forma de ejecutar el código es la siguiente:\n",
    "\n",
    "**python kafka_producer.py 0.1 0.3 Quatar_GP_2014 data/DATASETMotoGP-Qatar.csv**\n",
    "\n",
    "Antes de empezar a ejecutar el resto de la práctica, debemos definir el topic de lectura (mismo que el de escritura del producer) y la ip y puerto dónde se encuentre kafka (en nuestro caso en local, localhost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic = 'Quatar_GP_2014'\n",
    "kafkaBrokerIPPort = 'localhost:9092'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cuestiones\n",
    "\n",
    "IMPORTANTE: En cada momento sólo podemos tener activo un sparkContext. El modo de ejecución de las cuestiones es el siguiente:\n",
    "\n",
    "    1. Ejecutar cuadro con código de respuesta a la query\n",
    "    2. Ejecutar el comando de activacióń del streaming, \"scc.start\"\n",
    "    3. Ejecutar el comando de desactivación del streamin, \"scc.stop(False)\"\n",
    "\n",
    "En caso de experimentar problemas, reiniciar el kernel del notebook y ejecutar de nuevo la query en cuestión.\n",
    "\n",
    "##### A) Calcular el número total de menciones recibidas por cada cuenta de usuario durante el intervalo de 5 segundos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Context\n",
    "time_interval = 5\n",
    "\n",
    "sc = spark.sparkContext\n",
    "ssc = StreamingContext(sc, time_interval)\n",
    "\n",
    "kafkaParams = {\"metadata.broker.list\": kafkaBrokerIPPort}\n",
    "stream = KafkaUtils.createDirectStream(ssc, [topic], kafkaParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "info_row = stream.map(lambda line: line[1].split('\\t'))\n",
    "mentions_name = info_row.flatMap(lambda mentions: mentions[3].split(','))\n",
    "mention_map_names = mentions_name.map(lambda name: (name, 1) if name != '' else (name, 0))\n",
    "mention_count = mention_map_names.transform(lambda rdd: rdd.filter(lambda x: x[1] >0)) \\\n",
    "                                            .reduceByKey(lambda c1, c2: c1 + c2)\n",
    "mention_count.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2017-06-19 16:43:30\n",
      "-------------------------------------------\n",
      "('motogp', 2)\n",
      "('26_danipedrosa', 3)\n",
      "('nickyhayden', 1)\n",
      "('lorenzo99', 2)\n",
      "('marcmarquez93', 1)\n",
      "('calcrutchlow', 1)\n",
      "('senaldeportes', 1)\n",
      "('dcabellor', 1)\n",
      "('valeyellow46', 2)\n",
      "('citytv', 1)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-06-19 16:43:35\n",
      "-------------------------------------------\n",
      "('yonny68', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-06-19 16:43:40\n",
      "-------------------------------------------\n",
      "('christiandelbel', 1)\n",
      "('nickyhayden', 1)\n",
      "('valeyellow46', 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssc.stop(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B) Calcular la frecuencia total acumulada de apariciones de cada hashtag en el campo body, actualizando un ranking con los 5 hashtags con mayor frecuencia de aparición.\n",
    "\n",
    "\n",
    "<font color='red'>IMPORTANTE: En caso de fallo en tiempo de ejecución, reiniciar Kernel de jupyter(Kernal -> Restart) y la query funcionará </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Context\n",
    "time_interval = 5\n",
    "rank_hast = 5\n",
    "\n",
    "sc = spark.sparkContext\n",
    "ssc = StreamingContext(sc, time_interval)\n",
    "\n",
    "kafkaParams = {\"metadata.broker.list\": kafkaBrokerIPPort}\n",
    "stream = KafkaUtils.createDirectStream(ssc, [topic], kafkaParams)\n",
    "\n",
    "def updateFunction(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "        runningCount = 0    \n",
    "    return sum(newValues,runningCount)\n",
    "\n",
    "def checkpoint_dir(check_directory):\n",
    "    if os.path.exists(check_directory):\n",
    "        shutil.rmtree(check_directory)    \n",
    "    os.makedirs(check_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_directory = 'checkpoint'\n",
    "checkpoint_dir(check_directory)\n",
    "ssc.checkpoint(check_directory)\n",
    "\n",
    "rows = stream.map(lambda line: line[1].split('\\t'))\n",
    "body = rows.flatMap(lambda row: row[6].split(' '))\n",
    "hashtags= body.map(lambda word: (word, 1) if word.startswith('#') else (word, 0))\n",
    "hashtags_count = hashtags.transform(lambda rdd: rdd.filter(lambda hash: hash[1] > 0)\\\n",
    "                                   .map(lambda item: (item[0], item[1])))\\\n",
    "                                    .updateStateByKey(updateFunction)                                           \n",
    "            \n",
    "top_rank = hashtags_count.transform(lambda rdd: rdd.sortBy(lambda tup: tup[1], ascending = False)\n",
    "                                   .map(lambda ord_tup: list(ord_tup)).zipWithIndex()\n",
    "                                   .filter(lambda ranked_tup: ranked_tup[1] < rank_hast)\n",
    "                                   .map(lambda filt_tup: (filt_tup[0][0], filt_tup[0][1])))                                    \n",
    "                                 \n",
    "\n",
    "top_rank.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2017-06-19 16:44:30\n",
      "-------------------------------------------\n",
      "('#f1', 14)\n",
      "('#gpmalasia', 3)\n",
      "('#motogp', 2)\n",
      "('#sepangcircuit', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-06-19 16:44:35\n",
      "-------------------------------------------\n",
      "('#f1', 34)\n",
      "('#gpmalasia', 6)\n",
      "('#motogp', 6)\n",
      "('#defrenteconeldeporte', 3)\n",
      "('#f1/maldonado', 3)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-06-19 16:44:40\n",
      "-------------------------------------------\n",
      "('#f1', 35)\n",
      "('#f1/maldonado', 11)\n",
      "('#defrenteconeldeporte', 10)\n",
      "('#gpmalasia', 6)\n",
      "(\"#miiguel'dfultree\", 6)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssc.stop(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C) Calcular en una ventana temporal 20 segundos con offset de 10 segundos la frecuencia de aparición de cada uno de los 3 posibles tipos de tweets (TW-RT-MT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Context\n",
    "time_interval = 5\n",
    "time_window = 20\n",
    "offset = 10\n",
    "\n",
    "sc = spark.sparkContext\n",
    "ssc = StreamingContext(sc, time_interval)\n",
    "\n",
    "kafkaParams = {\"metadata.broker.list\": kafkaBrokerIPPort}\n",
    "stream = KafkaUtils.createDirectStream(ssc, [topic], kafkaParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "info_row = stream.map(lambda line: line[1].split('\\t'))\n",
    "\n",
    "tweet_types = info_row.map(lambda line: (line[9], 1))\n",
    "frequency  =  tweet_types.reduceByKeyAndWindow(add, sub,\n",
    "                                               windowDuration = time_window,\n",
    "                                               slideDuration = offset)\n",
    "frequency.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2017-06-19 16:45:00\n",
      "-------------------------------------------\n",
      "('MT', 14)\n",
      "('TW', 33)\n",
      "('RT', 7)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-06-19 16:45:10\n",
      "-------------------------------------------\n",
      "('MT', 37)\n",
      "('TW', 54)\n",
      "('RT', 14)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-06-19 16:45:20\n",
      "-------------------------------------------\n",
      "('MT', 57)\n",
      "('TW', 21)\n",
      "('RT', 23)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssc.stop(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
